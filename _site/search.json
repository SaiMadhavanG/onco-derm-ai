[
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "Initialization",
    "section": "",
    "text": "Initialization\n\ncome up with a name\nkedro init\npre commit hooks\n\nlinting\ntesting\nquartodoc\n\ntesting - pytest, git pre commit hook\nauto documentation - quartodoc\n\n\n\nDocumentation\n\nproject card - sai\ndata card\nmodel card\nmlops card\n\n\n\nData Prep\n\nLoad data - kedro\nData versioning - kedro\nImage preprocessing - normalizing, tensorizing and resizing - torchvision - srini\nData augmentation - torchvision\n\nData quality - ?, greater expectation\nright to erasure, forgetting - kedro pipeline\n\n\n\n\nTraining\n\nModel training - Resnet - PyTorch\nModel eval - Sk classification report\nModel versioning - mlflow\nhyperparameter tuning - optuna/sklearn search\nautomatic reports - quarto and plotting libs\nModel pruning - pytorch\nadversarial robustness - auto_lirpa\nright to erasure, model retraining - kedro pipeline\n\n\n\nInference\n\nexplainability - deel\nOOD detection - pytorch ood\nConformal predictions - deel\n\n\n\nDeployment\n\nmodel containerization - mlflow, docker\nauto model deployment - github actions/cloud provider\ndeployment side eval\ndata drift\nauto retraining triggers\n\n\n\nLLM\n\nllm set up\nllm prompt config\nllm deployment\n\n\n\nFront End\n\nFrontend dashboard - react, bootstrap\nChatbot window"
  },
  {
    "objectID": "notebooks/RestNet.html",
    "href": "notebooks/RestNet.html",
    "title": "Fine tune Resnet-18",
    "section": "",
    "text": "This code is for fine-tuning the ResNet model on the DermaMnist dataset.\nThe DermaMnist dataset is a collection of dermatological images used for skin disease classification. Fine-tuning involves taking a pre-trained ResNet model and adapting it to the specific task of classifying images in the DermaMnist dataset.\nThe process typically includes: - Loading the pre-trained ResNet model. - Modifying the final layers to match the number of classes in the DermaMnist dataset. - Training the modified model on the DermaMnist dataset. - Evaluating the performance of the fine-tuned model.\nThis approach leverages the pre-trained features of the ResNet model, which can lead to better performance and faster convergence compared to training a model from scratch.\n\n# import torch\n# import torchvision.models as models\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\n\n# Load the pre-trained ResNet-18 model\nresnet18 = models.resnet18(pretrained=True)\n\n# Print the model architecture\n# print(resnet18.fc)\n\n\nclass DermaMNISTDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        image = self.dataframe[\"image\"][idx]  # (28, 28, 3) numpy array\n        # image = (image * 255).astype(np.uint8)  # Convert to uint8 for transforms\n        label = self.dataframe[\"label\"][idx]\n\n        # Convert numpy image to PIL Image for applying transforms\n        image = transforms.ToPILImage()(image)\n        image = transforms.ToTensor()(image)\n        if self.transform:\n            image = self.transform(image)\n\n        # print(type(label))\n        # print(type(image))\n        return image, label\n\n\n# %reload_ext kedro.ipython\n\n[11/08/24 22:03:27] INFO     Registered line magic '%reload_kedro'                                   __init__.py:58\n\n\n\n                    INFO     Registered line magic '%load_node'                                      __init__.py:60\n\n\n\n                    INFO     Resolved project path as: c:\\Users\\Admin\\Desktop\\onco-derm-ai.         __init__.py:175\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n                    INFO     Registering new custom resolver: 'km.random_name'                    mlflow_hook.py:62\n\n\n\n                    WARNING  No 'mlflow.yml' config file found in environment. Default            mlflow_hook.py:75\n                             configuration will be used. Use ``kedro mlflow init`` command in CLI                  \n                             to customize the configuration.                                                       \n\n\n\n                    INFO     The 'tracking_uri' key in mlflow.yml is relative            kedro_mlflow_config.py:260\n                             ('server.mlflow_(tracking|registry)_uri = mlruns'). It is                             \n                             converted to a valid uri:                                                             \n                             'file:///C:/Users/Admin/Desktop/onco-derm-ai/mlruns'                                  \n\n\n\n[11/08/24 22:03:28] INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/08/24 22:03:29] INFO     Kedro project onco-derm-ai                                             __init__.py:141\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:142\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:148\n\n\n\n\nfrom kedro.io.data_catalog import DataCatalog\n\ncatalog = DataCatalog.from_config(\"../conf/base/catalog.yml\")\n\n\n# catalog\n\n\n\n\n\n{'train_dataset': 'kedro.io.memory_dataset.MemoryDataset()',\n 'train_intermediate': 'kedro.io.memory_dataset.MemoryDataset()',\n 'train_raw': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/train.pkl'), \"\n              \"backend='pickle', protocol='file', load_args={}, save_args={}, \"\n              \"version=Version(load=None, save='2024-11-08T16.21.06.976Z'))\",\n 'val_raw': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/val.pkl'), \"\n            \"backend='pickle', protocol='file', load_args={}, save_args={}, \"\n            \"version=Version(load=None, save='2024-11-08T16.21.06.976Z'))\",\n 'test_raw': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/test.pkl'), \"\n             \"backend='pickle', protocol='file', load_args={}, save_args={}, \"\n             \"version=Version(load=None, save='2024-11-08T16.21.06.976Z'))\",\n 'pre-processed_train_data': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/02_intermediate/pre-processed_train_data.pkl'), \"\n                             \"backend='pickle', protocol='file', load_args={}, \"\n                             'save_args={}, version=Version(load=None, '\n                             \"save='2024-11-08T16.21.06.976Z'))\",\n 'image_classification_model': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/models/image_classification_model.pkl'), \"\n                               \"backend='pickle', protocol='file', \"\n                               'load_args={}, save_args={}, '\n                               'version=Version(load=None, '\n                               \"save='2024-11-08T16.21.06.976Z'))\",\n 'model_finetuned': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/models/model_fintuned.pkl'), \"\n                    \"backend='pickle', protocol='file', load_args={}, \"\n                    'save_args={}, version=Version(load=None, '\n                    \"save='2024-11-08T16.21.06.976Z'))\",\n 'parameters': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;dict&gt;')\",\n 'params:model_name': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;str&gt;')\",\n 'params:num_epochs': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;int&gt;')\"}\n\n\n\n\ntrain_data = catalog.load(\"pre-processed_train_data\")\n\n[11/08/24 22:03:40] INFO     Loading data from pre-processed_train_data (PickleDataset)...      data_catalog.py:539\n\n\n\n\n# train_data[\"image\"][0]\n\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),  # Resize to 224x224\n        # transforms.ToTensor(),             # Convert to tensor\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        ),  # Normalize\n    ]\n)\n\n# Load your dataset\ntrain_dataset = DermaMNISTDataset(train_data, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Initialize ResNet-18 model\nmodel = models.resnet18(pretrained=True)\nnum_classes = 7\nmodel.fc = nn.Linear(\n    model.fc.in_features, num_classes\n)  # Adjust final layer for DermaMNIST classes\n\n# Set up loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nnum_epochs = 10  # Adjust as needed\n\n# train_loader\n\n[11/08/24 22:03:45] WARNING  c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\torchvision\\mod warnings.py:109\n                             els\\_utils.py:208: UserWarning: The parameter 'pretrained' is                         \n                             deprecated since 0.13 and may be removed in the future, please use                    \n                             'weights' instead.                                                                    \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\torchvision\\mod warnings.py:109\n                             els\\_utils.py:223: UserWarning: Arguments other than a weight enum or                 \n                             `None` for 'weights' are deprecated since 0.13 and may be removed in                  \n                             the future. The current behavior is equivalent to passing                             \n                             `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use                            \n                             `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.                \n                               warnings.warn(msg)                                                                  \n                                                                                                                   \n\n\n\n\nimages = \"\"\nlabels = \"\"\noutputs = \"\"\nlost = \"\"\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, lbs in train_loader:\n        # print(type(images))\n        images, labels = i.to(device), lbs.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        # print(labels)\n        labels_output = labels.squeeze().long()\n        loss = criterion(outputs, labels_output)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n    # Save the model checkpoint\ntorch.save(model.state_dict(), \"resnet18_dermamnist.pth\")\n# print(\"Pre-training complete.\")\n\nEpoch [1/10], Loss: 0.8958\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:19                                                                                   │\n│                                                                                                  │\n│   16 │   │   loss.backward()                                                                     │\n│   17 │   │   optimizer.step()                                                                    │\n│   18 │   │                                                                                       │\n│ ❱ 19 │   │   running_loss += loss.item()                                                         │\n│   20 │                                                                                           │\n│   21 │   print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")    │\n│   22 │   # Save the model checkpoint                                                             │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nKeyboardInterrupt"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "OncoDerm AI",
    "section": "",
    "text": "Project Proposal\n\nExecutive Summary\nWe propose developing a machine learning-based skin cancer screening system utilizing dermatoscopic images. The system will assist medical professionals in preliminary skin lesion assessment while incorporating robust MLOps practices, responsible AI principles, and an interactive chatbot feature powered by a Large Language Model (LLM) for enhanced user engagement.\n\n\nProblem Statement\nSkin cancer diagnosis requires expert dermatological knowledge and careful image analysis. While machine learning can assist in this process, deploying such systems in clinical settings requires careful consideration of reliability, explainability, operational excellence, and ease of interaction for medical professionals.\n\n\nDataset\n\nSource: DermaMNIST (based on HAM10000)\nClasses: 7 distinct skin lesion categories:\n\nActinic keratoses and intraepithelial carcinoma (akiec)\nBasal cell carcinoma (bcc)\nBenign keratosis-like lesions (bkl)\nDermatofibroma (df)\nMelanoma (mel)\nMelanocytic nevi (nv)\nVascular lesions (vasc)\n\nCharacteristics:\n\n28x28 pixel dermatoscopic images\n10,015 training images\n1,268 validation images\n2,239 test images\n\nData Split: Predefined splits provided by MedMNIST\n\n\n\nTechnical Architecture\n\n1. Model Development\n\nBase Architecture:\n\nResNet-18 or MobileNetV2 (modified for 28x28 input)\nLightweight models for compatibility with small input size\n\nTraining Pipeline:\n\nData augmentation (rotation, flipping, color jittering)\nTransfer learning with ImageNet weights\nFine-tuning for small images\nCross-validation for robust performance estimation\n\n\n\n\n2. MLOps Infrastructure\n\nData Pipeline:\n\nData ingestion and preprocessing\nData versioning\nData quality checks\n\nExperiment Tracking:\n\nMLflow for model versioning and tracking\nHyperparameter optimization\nModel performance visualization\n\nCI/CD Pipeline:\n\nAutomated testing (unit, integration, model performance)\nAutomated model deployment\nAutomated documentation generation\n\nMonitoring:\n\nModel performance metrics\nData drift detection\nAutomatic retraining triggers\n\n\n\n\n3. Production Features\n\nModel Robustness & Reliability\n\nExplainability\nConfidence Calibration\nAdversarial Robustness\nOut-of-Distribution Detection\n\n\n\nChatbot Integration with LLM\n\nInteractive Chatbot: We will integrate a chatbot powered by an LLM to enable natural language interactions with users.\n\nFunctionality: The chatbot will provide explanations, clarifications, and further details about the model’s prediction, confidence score, and lesion category. Medical professionals can ask follow-up questions, discuss specific cases, and obtain interpretative guidance.\nImplementation: All model outputs (prediction, confidence score, and explainability results) will be passed as inputs to the LLM, enabling contextual and conversational responses based on real-time model data.\nBenefits: Allows users to interact in natural language, promoting accessibility and understanding, especially useful for non-technical users in clinical settings.\n\n\n\n\nClinical Integration\n\nInteractive Dashboard:\n\nReal-time inference results\nConfidence scores and explanations\nImage preprocessing and quality checks\nResolution handling and upscaling options\n\nConformal Predictions:\n\nSet-valued predictions with guaranteed coverage\nCalibrated confidence scores\n\n\n\n\nData Privacy & Compliance\n\nRight to Erasure:\n\nAutomated removal pipeline\n\n\n\n\n\n\nEvaluation Metrics\n\nTechnical Metrics\n\nModel accuracy\nPrecision, recall, F1-score per class\nInference latency\nData drift metrics\n\n\n\nClinical Metrics\n\nFalse positive/negative rates\nCalibration error\nOOD detection accuracy\nExplanation quality (user feedback from chatbot interactions)\n\n\n\n\nChallenges & Risks\n\nTechnical Risks:\n\nLimited resolution impact on performance\nModel bias\nSystem scalability\nIntegration challenges\n\nClinical Risks:\n\nOver-reliance on system\nMisinterpretation of results\nEdge case handling\nResolution limitations affecting diagnosis\n\n\n\n\nMitigation Strategies\n\nClear Disclaimer: System is for screening assistance only\nResolution Warning: Clear indication of image resolution limitations\nComprehensive Documentation: Usage guidelines and limitations\nRegular Updates: Continuous model improvement\nUser Training: Proper system usage and interpretation"
  },
  {
    "objectID": "docs/cards/data-card.html",
    "href": "docs/cards/data-card.html",
    "title": "DermaMNIST Dataset Card",
    "section": "",
    "text": "Using downloaded and verified file: /home/saimadhavang/.medmnist/dermamnist.npz"
  },
  {
    "objectID": "docs/cards/data-card.html#purpose-of-collection",
    "href": "docs/cards/data-card.html#purpose-of-collection",
    "title": "DermaMNIST Dataset Card",
    "section": "Purpose of Collection",
    "text": "Purpose of Collection\nThe DermaMNIST dataset is collected to aid in the development and evaluation of machine learning models for skin lesion classification. Its primary application is in dermatological research and clinical diagnostics."
  },
  {
    "objectID": "docs/cards/data-card.html#collection-timeline",
    "href": "docs/cards/data-card.html#collection-timeline",
    "title": "DermaMNIST Dataset Card",
    "section": "Collection Timeline",
    "text": "Collection Timeline\n\nCollection Period: 2020 (part of the MedMNIST collection)\nVersion: 2.1.0"
  },
  {
    "objectID": "docs/cards/data-card.html#data-collectors-and-stakeholders",
    "href": "docs/cards/data-card.html#data-collectors-and-stakeholders",
    "title": "DermaMNIST Dataset Card",
    "section": "Data Collectors and Stakeholders",
    "text": "Data Collectors and Stakeholders\n\nPrincipal Investigators: Jiancheng Yang, et al.\nInstitution: Shanghai Jiao Tong University\nRepository: MedMNIST (GitHub: medmnist/medmnist)\nMaintainer: MedMNIST team"
  },
  {
    "objectID": "docs/cards/data-card.html#dataset-overview",
    "href": "docs/cards/data-card.html#dataset-overview",
    "title": "DermaMNIST Dataset Card",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\nFormat: .npz (NumPy compressed)\nData Type: Image-based (RGB, 28x28 resolution)\nStorage Requirements: Approx. 13 MB"
  },
  {
    "objectID": "docs/cards/data-card.html#sensitive-data-assessment",
    "href": "docs/cards/data-card.html#sensitive-data-assessment",
    "title": "DermaMNIST Dataset Card",
    "section": "Sensitive Data Assessment",
    "text": "Sensitive Data Assessment\nThe dataset contains de-identified medical images of skin lesions. While there is no personal health information (PHI), users are advised to follow ethical guidelines for medical data usage."
  },
  {
    "objectID": "docs/cards/data-card.html#recommended-applications",
    "href": "docs/cards/data-card.html#recommended-applications",
    "title": "DermaMNIST Dataset Card",
    "section": "Recommended Applications",
    "text": "Recommended Applications\n\nImage Classification Models: For lesion detection and diagnosis.\nBenchmarking: A standard benchmark for medical imaging tasks.\nEducational Use: Ideal for teaching machine learning in medical contexts."
  },
  {
    "objectID": "docs/cards/data-card.html#known-limitations",
    "href": "docs/cards/data-card.html#known-limitations",
    "title": "DermaMNIST Dataset Card",
    "section": "Known Limitations",
    "text": "Known Limitations\n\nLimited to 28x28 resolution, which may constrain advanced model performance."
  },
  {
    "objectID": "docs/cards/project-card.html",
    "href": "docs/cards/project-card.html",
    "title": "Project Card",
    "section": "",
    "text": "Provide succinct background to the problem so that the reader can empathize with the problem.\nSkin cancer is a growing concern worldwide, and in rural India, the lack of accessible dermatological care creates significant barriers to early diagnosis and treatment. Many rural communities face a shortage of dermatologists, with patients often traveling long distances to receive specialized care. Without timely diagnosis, skin cancer cases may go undetected or be identified too late, impacting patient outcomes.\n\n\n\nWhat is the problem being solved?\nThe problem being solved is the lack of accessible, early skin cancer screening and diagnosis in rural India, where dermatologists and specialized medical resources are scarce. Patients in these regions often struggle to receive timely assessments, which can delay diagnosis and treatment of skin cancers and other serious skin conditions.\n\n\n\nWho it is for? Is that a _user or a beneficiary? What is the problem being solved? Who it is for?_\nOncoDerm AI is designed for healthcare providers in rural India who lack access to specialized dermatological support. The primary users of the system are non-specialist healthcare workers, such as general practitioners, nurses, and community health workers, who can utilize the tool to make preliminary skin cancer assessments. These healthcare workers rely on OncoDerm AI to screen patients for potential skin lesions, helping them identify high-risk cases that require further specialist care.\nThe beneficiaries of this project are the patients in these rural communities. By receiving timely and accessible screening, they gain a greater chance for early detection and treatment of skin cancer, which can improve outcomes and save lives.\n\n\n\nWhy it needs to be solved?\nThe need for OncoDerm AI arises from the critical gap in dermatological care in rural India, where access to early diagnosis and specialist support is limited. Skin cancer, when detected early, can often be treated successfully; however, without timely screening, cases may go undiagnosed until they reach advanced stages, significantly affecting patient outcomes and increasing healthcare costs.\nBy solving this problem, OncoDerm AI aims to improve early detection rates, support healthcare providers in delivering higher-quality care, and ultimately enhance health outcomes for underserved populations. This tool brings reliable, cost-effective diagnostic support to rural areas, where it’s most needed, helping to bridge the healthcare gap for vulnerable communities.\n\n\n\nHow does the solution look like? It is more of the experience, rather how it will be developed.\nOncoDerm AI is an AI-powered skin cancer screening tool designed to provide healthcare workers in rural India with the ability to assess skin lesions using dermatoscopic images. The product consists of the following key components:\n\nAI Model: The core of the system is a deep learning model trained on a large dataset of dermatoscopic images (DermaMNIST), capable of classifying skin lesions into one of seven categories, including melanoma, basal cell carcinoma, and benign conditions. The model outputs predictions along with confidence scores and explanations for each classification.\nInteractive Chatbot: To enhance user engagement and accessibility, OncoDerm AI features a chatbot powered by a Large Language Model (LLM). Healthcare workers can interact with the chatbot to ask follow-up questions, seek clarifications, and obtain detailed explanations of the model’s predictions. The chatbot will provide context, offer interpretive guidance, and help users understand the significance of the results in simple language, making it easier for non-specialist users to interpret complex AI outputs.\nInteractive Dashboard: The tool provides a user-friendly dashboard that displays real-time skin lesion assessments. The dashboard includes:\n\nPredictions with confidence scores\nExplanations of the model’s decision-making process\nImage preprocessing and quality checks\nOptions for resolution handling and upscaling to ensure clarity of images\n\nClinical Integration: OncoDerm AI can be integrated into the existing healthcare workflows in rural clinics, making it easy to use without the need for specialized equipment. Healthcare workers can upload dermatoscopic images via mobile devices or computers, receive immediate results, and engage in a conversation with the chatbot for further guidance.\nData Privacy & Compliance: OncoDerm AI ensures data privacy, featuring an automated data removal pipeline to handle the right to erasure requests from patients.\n\nThe experience for the user is seamless: after uploading a skin lesion image, the healthcare worker receives immediate feedback from the AI model along with an explanation of the diagnosis. If they need further clarification, they can interact with the chatbot to receive more detailed information, enhancing their understanding and confidence in the decision-making process.\nOncoDerm AI is designed to be easy to use, ensuring that healthcare providers in rural areas, with varying levels of technical expertise, can effectively use it to support early skin cancer detection.\n\n\n\nBreakdown the product into key (business) objectives that need to be delivered? SMART Goals is useful to frame\nThe OncoDerm AI project aims to deliver a comprehensive solution for skin cancer screening in rural India, where access to dermatologists is limited. Below are the key business objectives, framed as SMART goals:\n\nObjective 1: Achieve 85% Model Accuracy in Skin Lesion Classification\n\nSpecific: Develop and deploy an AI model that classifies skin lesions into 7 categories (including melanoma and basal cell carcinoma) with a minimum accuracy of 85%.\nMeasurable: Accuracy will be evaluated using the validation set (1,268 images) and tested on the test set (2,239 images).\nAchievable: Using a well-known architecture like ResNet-18 or MobileNetV2, and leveraging transfer learning and data augmentation.\nRelevant: High classification accuracy is critical to ensuring the system provides reliable results to healthcare workers.\nTime-bound: Achieve this goal within end of semseter.\n\nObjective 2: Integrate an Interactive Chatbot for Enhanced User Engagement\n\nSpecific: Integrate a chatbot powered by an LLM that allows users to interact with the system, asking for clarifications and receiving model predictions, confidence scores, and explanations.\nMeasurable: Measure user engagement through usage statistics, and collect feedback on chatbot helpfulness through user surveys.\nAchievable: Integrating a pre-trained LLM and providing a user-friendly interface for querying the model.\nRelevant: The chatbot will help medical professionals in rural areas, who may not have in-depth dermatological knowledge, interpret the model’s results more confidently.\nTime-bound: Achieve this goal within end of semseter.\n\nObjective 3: Ensure Data Privacy Compliance and Implement Right to Erasure\n\nSpecific: Implement a data pipeline that ensures patient data is securely handled, with a fully automated removal process for data erasure requests, in compliance with privacy laws and regulations.\nMeasurable: Successfully process at least 99% of data removal requests within 24 hours.\nAchievable: Design and implement an automated data removal pipeline that complies with local privacy standards.\nRelevant: Ensuring patient data privacy is crucial in gaining trust among users, particularly in rural settings where data sensitivity is a concern.\nTime-bound: Achieve this goal within end of semseter.\n\nObjective 5: Monitor System Performance and Retrain Model Every 3 Months\n\nSpecific: Implement a system for continuous monitoring of model performance and retrain the model every 3 months to ensure its accuracy and adapt to any changes in data (e.g., new skin lesion patterns).\nMeasurable: Measure model performance using metrics such as accuracy, precision, and recall, and retrain when performance degrades by more than 5%.\nAchievable: Set up continuous integration and delivery (CI/CD) pipelines to monitor performance and trigger retraining as needed.\nRelevant: Ensuring the model stays up-to-date and reliable is essential for maintaining trust and accuracy in the system.\nTime-bound: Implement continuous monitoring and retraining pipeline within 3 months of initial deployment, and retrain every 3 months thereafter.\n\n\n\n\n\nWhat are the challenges one can face and ways to overcome?\n\n\n\nDeveloping and deploying the OncoDerm AI system in rural India presents several challenges. Here are the key risks and potential mitigation strategies:\n\nLimited Image Resolution and Dataset Size\n\nChallenge: The DermaMNIST dataset contains 28x28 pixel images, which may limit the model’s ability to detect subtle visual features. Additionally, the dataset is relatively small, which can lead to overfitting.\nMitigation: Use transfer learning with pre-trained models on higher-resolution dermatoscopic images to improve feature extraction. Employ data augmentation techniques (e.g., rotation, flipping, color jittering) to create a more diverse dataset. Continuously update and expand the dataset by incorporating new data from partner clinics to improve model robustness.\n\nModel Interpretability and User Trust\n\nChallenge: Users in rural areas, often non-specialist healthcare workers, may lack the medical background to interpret complex AI outputs, which could lead to mistrust or misuse.\nMitigation: Provide clear, interpretable model outputs and confidence scores. Integrate the LLM-powered chatbot to provide accessible explanations, allowing users to ask questions and understand predictions better. Implement conformal predictions to communicate confidence in a way that’s easy to understand (e.g., “high confidence” vs. “low confidence”).\n\nTechnical Infrastructure Constraints in Rural Areas\n\nChallenge: Rural clinics may face limited access to high-speed internet, modern hardware, or reliable electricity, affecting system deployment and performance.\nMitigation: Optimize the model for low-resource environments by using lightweight architectures (e.g., MobileNetV2) and deploying the model on local devices with minimal computational requirements. Consider offline capabilities and integrate power backups if feasible.\n\nPrivacy and Data Security Concerns\n\nChallenge: Patient data privacy is crucial, particularly in sensitive areas like healthcare. Rural clinics may also have limited understanding of data privacy practices.\nMitigation: Implement data encryption, secure access controls, and compliance with local privacy laws. Develop training sessions for clinic staff on data privacy best practices. Set up an automated data removal pipeline to handle right-to-erasure requests efficiently, and regularly review compliance.\n\nModel Drift and Data Distribution Shifts\n\nChallenge: Skin lesion data may change over time due to environmental, genetic, or treatment factors, leading to model drift.\nMitigation: Implement continuous monitoring for data drift and set up a CI/CD pipeline to retrain the model as needed. Regularly assess model accuracy, and retrain every 3 months or when performance drops significantly. Collect user feedback and periodic clinical validation to ensure the model remains relevant.\n\nUser Adoption and Training\n\nChallenge: Local healthcare workers may be unfamiliar with AI systems, and mistrust or a lack of training may reduce adoption.\nMitigation: Collaborate with healthcare providers and NGOs to train healthcare workers on using the system effectively, including hands-on demonstrations and support materials in regional languages. Establish a support team for ongoing assistance, and offer a simplified user interface with a clear workflow to increase usability and confidence."
  },
  {
    "objectID": "docs/cards/project-card.html#background",
    "href": "docs/cards/project-card.html#background",
    "title": "Project Card",
    "section": "",
    "text": "Provide succinct background to the problem so that the reader can empathize with the problem.\nSkin cancer is a growing concern worldwide, and in rural India, the lack of accessible dermatological care creates significant barriers to early diagnosis and treatment. Many rural communities face a shortage of dermatologists, with patients often traveling long distances to receive specialized care. Without timely diagnosis, skin cancer cases may go undetected or be identified too late, impacting patient outcomes."
  },
  {
    "objectID": "docs/cards/project-card.html#problem",
    "href": "docs/cards/project-card.html#problem",
    "title": "Project Card",
    "section": "",
    "text": "What is the problem being solved?\nThe problem being solved is the lack of accessible, early skin cancer screening and diagnosis in rural India, where dermatologists and specialized medical resources are scarce. Patients in these regions often struggle to receive timely assessments, which can delay diagnosis and treatment of skin cancers and other serious skin conditions."
  },
  {
    "objectID": "docs/cards/project-card.html#customer",
    "href": "docs/cards/project-card.html#customer",
    "title": "Project Card",
    "section": "",
    "text": "Who it is for? Is that a _user or a beneficiary? What is the problem being solved? Who it is for?_\nOncoDerm AI is designed for healthcare providers in rural India who lack access to specialized dermatological support. The primary users of the system are non-specialist healthcare workers, such as general practitioners, nurses, and community health workers, who can utilize the tool to make preliminary skin cancer assessments. These healthcare workers rely on OncoDerm AI to screen patients for potential skin lesions, helping them identify high-risk cases that require further specialist care.\nThe beneficiaries of this project are the patients in these rural communities. By receiving timely and accessible screening, they gain a greater chance for early detection and treatment of skin cancer, which can improve outcomes and save lives."
  },
  {
    "objectID": "docs/cards/project-card.html#value-proposition",
    "href": "docs/cards/project-card.html#value-proposition",
    "title": "Project Card",
    "section": "",
    "text": "Why it needs to be solved?\nThe need for OncoDerm AI arises from the critical gap in dermatological care in rural India, where access to early diagnosis and specialist support is limited. Skin cancer, when detected early, can often be treated successfully; however, without timely screening, cases may go undiagnosed until they reach advanced stages, significantly affecting patient outcomes and increasing healthcare costs.\nBy solving this problem, OncoDerm AI aims to improve early detection rates, support healthcare providers in delivering higher-quality care, and ultimately enhance health outcomes for underserved populations. This tool brings reliable, cost-effective diagnostic support to rural areas, where it’s most needed, helping to bridge the healthcare gap for vulnerable communities."
  },
  {
    "objectID": "docs/cards/project-card.html#product",
    "href": "docs/cards/project-card.html#product",
    "title": "Project Card",
    "section": "",
    "text": "How does the solution look like? It is more of the experience, rather how it will be developed.\nOncoDerm AI is an AI-powered skin cancer screening tool designed to provide healthcare workers in rural India with the ability to assess skin lesions using dermatoscopic images. The product consists of the following key components:\n\nAI Model: The core of the system is a deep learning model trained on a large dataset of dermatoscopic images (DermaMNIST), capable of classifying skin lesions into one of seven categories, including melanoma, basal cell carcinoma, and benign conditions. The model outputs predictions along with confidence scores and explanations for each classification.\nInteractive Chatbot: To enhance user engagement and accessibility, OncoDerm AI features a chatbot powered by a Large Language Model (LLM). Healthcare workers can interact with the chatbot to ask follow-up questions, seek clarifications, and obtain detailed explanations of the model’s predictions. The chatbot will provide context, offer interpretive guidance, and help users understand the significance of the results in simple language, making it easier for non-specialist users to interpret complex AI outputs.\nInteractive Dashboard: The tool provides a user-friendly dashboard that displays real-time skin lesion assessments. The dashboard includes:\n\nPredictions with confidence scores\nExplanations of the model’s decision-making process\nImage preprocessing and quality checks\nOptions for resolution handling and upscaling to ensure clarity of images\n\nClinical Integration: OncoDerm AI can be integrated into the existing healthcare workflows in rural clinics, making it easy to use without the need for specialized equipment. Healthcare workers can upload dermatoscopic images via mobile devices or computers, receive immediate results, and engage in a conversation with the chatbot for further guidance.\nData Privacy & Compliance: OncoDerm AI ensures data privacy, featuring an automated data removal pipeline to handle the right to erasure requests from patients.\n\nThe experience for the user is seamless: after uploading a skin lesion image, the healthcare worker receives immediate feedback from the AI model along with an explanation of the diagnosis. If they need further clarification, they can interact with the chatbot to receive more detailed information, enhancing their understanding and confidence in the decision-making process.\nOncoDerm AI is designed to be easy to use, ensuring that healthcare providers in rural areas, with varying levels of technical expertise, can effectively use it to support early skin cancer detection."
  },
  {
    "objectID": "docs/cards/project-card.html#objectives",
    "href": "docs/cards/project-card.html#objectives",
    "title": "Project Card",
    "section": "",
    "text": "Breakdown the product into key (business) objectives that need to be delivered? SMART Goals is useful to frame\nThe OncoDerm AI project aims to deliver a comprehensive solution for skin cancer screening in rural India, where access to dermatologists is limited. Below are the key business objectives, framed as SMART goals:\n\nObjective 1: Achieve 85% Model Accuracy in Skin Lesion Classification\n\nSpecific: Develop and deploy an AI model that classifies skin lesions into 7 categories (including melanoma and basal cell carcinoma) with a minimum accuracy of 85%.\nMeasurable: Accuracy will be evaluated using the validation set (1,268 images) and tested on the test set (2,239 images).\nAchievable: Using a well-known architecture like ResNet-18 or MobileNetV2, and leveraging transfer learning and data augmentation.\nRelevant: High classification accuracy is critical to ensuring the system provides reliable results to healthcare workers.\nTime-bound: Achieve this goal within end of semseter.\n\nObjective 2: Integrate an Interactive Chatbot for Enhanced User Engagement\n\nSpecific: Integrate a chatbot powered by an LLM that allows users to interact with the system, asking for clarifications and receiving model predictions, confidence scores, and explanations.\nMeasurable: Measure user engagement through usage statistics, and collect feedback on chatbot helpfulness through user surveys.\nAchievable: Integrating a pre-trained LLM and providing a user-friendly interface for querying the model.\nRelevant: The chatbot will help medical professionals in rural areas, who may not have in-depth dermatological knowledge, interpret the model’s results more confidently.\nTime-bound: Achieve this goal within end of semseter.\n\nObjective 3: Ensure Data Privacy Compliance and Implement Right to Erasure\n\nSpecific: Implement a data pipeline that ensures patient data is securely handled, with a fully automated removal process for data erasure requests, in compliance with privacy laws and regulations.\nMeasurable: Successfully process at least 99% of data removal requests within 24 hours.\nAchievable: Design and implement an automated data removal pipeline that complies with local privacy standards.\nRelevant: Ensuring patient data privacy is crucial in gaining trust among users, particularly in rural settings where data sensitivity is a concern.\nTime-bound: Achieve this goal within end of semseter.\n\nObjective 5: Monitor System Performance and Retrain Model Every 3 Months\n\nSpecific: Implement a system for continuous monitoring of model performance and retrain the model every 3 months to ensure its accuracy and adapt to any changes in data (e.g., new skin lesion patterns).\nMeasurable: Measure model performance using metrics such as accuracy, precision, and recall, and retrain when performance degrades by more than 5%.\nAchievable: Set up continuous integration and delivery (CI/CD) pipelines to monitor performance and trigger retraining as needed.\nRelevant: Ensuring the model stays up-to-date and reliable is essential for maintaining trust and accuracy in the system.\nTime-bound: Implement continuous monitoring and retraining pipeline within 3 months of initial deployment, and retrain every 3 months thereafter."
  },
  {
    "objectID": "docs/cards/project-card.html#risks-challenges",
    "href": "docs/cards/project-card.html#risks-challenges",
    "title": "Project Card",
    "section": "",
    "text": "What are the challenges one can face and ways to overcome?"
  },
  {
    "objectID": "docs/cards/project-card.html#risks-challenges-1",
    "href": "docs/cards/project-card.html#risks-challenges-1",
    "title": "Project Card",
    "section": "",
    "text": "Developing and deploying the OncoDerm AI system in rural India presents several challenges. Here are the key risks and potential mitigation strategies:\n\nLimited Image Resolution and Dataset Size\n\nChallenge: The DermaMNIST dataset contains 28x28 pixel images, which may limit the model’s ability to detect subtle visual features. Additionally, the dataset is relatively small, which can lead to overfitting.\nMitigation: Use transfer learning with pre-trained models on higher-resolution dermatoscopic images to improve feature extraction. Employ data augmentation techniques (e.g., rotation, flipping, color jittering) to create a more diverse dataset. Continuously update and expand the dataset by incorporating new data from partner clinics to improve model robustness.\n\nModel Interpretability and User Trust\n\nChallenge: Users in rural areas, often non-specialist healthcare workers, may lack the medical background to interpret complex AI outputs, which could lead to mistrust or misuse.\nMitigation: Provide clear, interpretable model outputs and confidence scores. Integrate the LLM-powered chatbot to provide accessible explanations, allowing users to ask questions and understand predictions better. Implement conformal predictions to communicate confidence in a way that’s easy to understand (e.g., “high confidence” vs. “low confidence”).\n\nTechnical Infrastructure Constraints in Rural Areas\n\nChallenge: Rural clinics may face limited access to high-speed internet, modern hardware, or reliable electricity, affecting system deployment and performance.\nMitigation: Optimize the model for low-resource environments by using lightweight architectures (e.g., MobileNetV2) and deploying the model on local devices with minimal computational requirements. Consider offline capabilities and integrate power backups if feasible.\n\nPrivacy and Data Security Concerns\n\nChallenge: Patient data privacy is crucial, particularly in sensitive areas like healthcare. Rural clinics may also have limited understanding of data privacy practices.\nMitigation: Implement data encryption, secure access controls, and compliance with local privacy laws. Develop training sessions for clinic staff on data privacy best practices. Set up an automated data removal pipeline to handle right-to-erasure requests efficiently, and regularly review compliance.\n\nModel Drift and Data Distribution Shifts\n\nChallenge: Skin lesion data may change over time due to environmental, genetic, or treatment factors, leading to model drift.\nMitigation: Implement continuous monitoring for data drift and set up a CI/CD pipeline to retrain the model as needed. Regularly assess model accuracy, and retrain every 3 months or when performance drops significantly. Collect user feedback and periodic clinical validation to ensure the model remains relevant.\n\nUser Adoption and Training\n\nChallenge: Local healthcare workers may be unfamiliar with AI systems, and mistrust or a lack of training may reduce adoption.\nMitigation: Collaborate with healthcare providers and NGOs to train healthcare workers on using the system effectively, including hands-on demonstrations and support materials in regional languages. Establish a support team for ongoing assistance, and offer a simplified user interface with a clear workflow to increase usability and confidence."
  },
  {
    "objectID": "docs/cards/project-card.html#task",
    "href": "docs/cards/project-card.html#task",
    "title": "Project Card",
    "section": "Task",
    "text": "Task\nWhat type of prediction problem is this? Link Model Card when sufficient details become available (start small but early)\nThe prediction task for OncoDerm AI is a multi-class classification problem, where the model identifies one of seven distinct skin lesion types from dermatoscopic images. Using a labeled dataset (DermaMNIST), the model is trained to recognize each class based on visual features in low-resolution images (28x28 pixels), aiming to assist in preliminary diagnosis."
  },
  {
    "objectID": "docs/cards/project-card.html#metrics",
    "href": "docs/cards/project-card.html#metrics",
    "title": "Project Card",
    "section": "Metrics",
    "text": "Metrics\nHow will the solution be evaluated - What are the ML metrics? What are the business metrics? Link Model Card when sufficient details become available (start small but early)\n\nML Metrics\n\nAccuracy: Measures the model’s overall correctness in identifying the correct lesion category.\nPrecision, Recall, and F1-score (per class): Evaluates performance across each lesion type, ensuring balanced identification and minimizing false positives and false negatives, especially for critical classes like melanoma.\nOut-of-Distribution (OOD) Detection Accuracy: Measures the model’s ability to detect and flag cases it hasn’t been trained on, ensuring safer real-world application.\nInference Latency: Tracks the speed at which the model generates predictions to support smooth, real-time interactions in clinical settings.\n\n\n\nBusiness Metrics\n\nReduction in Referral Time: Measures time saved in identifying high-risk cases for further diagnosis, aiming to improve early detection rates.\nUser Engagement: Tracks interactions with the LLM-powered chatbot, measuring how often medical assistants use it for explanations, confidence clarification, and follow-up inquiries.\nFeedback from Rural Health Workers: Collect qualitative data on ease of use, clarity, and clinical effectiveness in providing primary assessments for skin lesions.\nSystem Usage Rate: Monitors adoption rates in rural clinics to ensure the model is accessible and practical for real-world needs."
  },
  {
    "objectID": "docs/cards/project-card.html#evaluation",
    "href": "docs/cards/project-card.html#evaluation",
    "title": "Project Card",
    "section": "Evaluation",
    "text": "Evaluation\nHow will the solution be evaluated (process)? Link Model Card when sufficient details become available (start small but early)\n\n1. Technical Evaluation\n\nOffline Testing:\n\nConduct rigorous testing on the DermaMNIST validation and test datasets to evaluate baseline accuracy, precision, recall, F1-score, and confidence calibration.\nPerform stress testing for out-of-distribution (OOD) detection using external images not present in the training data, ensuring the model can reliably flag unfamiliar or rare cases.\n\nHuman-in-the-Loop Validation:\n\nDermatologists will review a subset of model predictions, offering feedback on accuracy, confidence scores, and explanations provided by the chatbot, allowing for continuous refinement.\n\n\n\n\n2. Clinical Pilot Testing in Rural Clinics\n\nUser Acceptance Testing (UAT):\n\nDeploy the model in a few rural clinics to observe its usability, particularly with non-specialist health workers. Measure system usage, evaluate time savings in preliminary assessments, and record feedback on interpretability and clarity of results.\n\nReal-World Feedback Collection:\n\nIntegrate a feedback loop for rural health workers to share experiences using the chatbot, focusing on how well it assists in understanding and relaying predictions.\nAssess the reduction in referral time for high-risk cases, aiming to expedite diagnosis.\n\n\n\n\n3. Ongoing Monitoring\n\nContinuous Model Evaluation:\n\nTrack and log model performance metrics in real-time, including confidence scores, latency, and data drift, triggering automatic retraining if significant shifts are detected.\nCalibrate the chatbot’s responses based on feedback to ensure it effectively answers typical questions and addresses user needs."
  },
  {
    "objectID": "docs/cards/project-card.html#data",
    "href": "docs/cards/project-card.html#data",
    "title": "Project Card",
    "section": "Data",
    "text": "Data\nWhat type of data is needed? How will it be collected - for training and for continuous improvement? Link Data Cards when sufficient details become available (start small but early)\n\nData Requirements\n\nPrimary Data: Dermatoscopic images for training and validating the model.\n\nType: 28x28 pixel RGB images focused on different skin lesions.\nClasses: Seven distinct skin lesion types (e.g., melanoma, basal cell carcinoma).\n\nAuxiliary Data: Patient metadata (e.g., age, lesion location) may be incorporated if available to enhance prediction accuracy.\n\n\n\nData Collection\n\nTraining and Initial Evaluation:\n\nDataset: DermaMNIST (based on the HAM10000 dataset), which provides a comprehensive set of images from diverse patients.\nSplits: Predefined training, validation, and test splits will be used to ensure consistency and reliability in performance evaluation.\n\nContinuous Improvement:\n\nUser-Generated Data: Images from new cases in rural clinics can be anonymized and added to expand and adapt the model to real-world conditions.\nFeedback Loop: Health workers’ and dermatologists’ feedback on chatbot interactions, predictions, and flagged OOD cases can further refine the system."
  },
  {
    "objectID": "docs/cards/project-card.html#continuous-improvement",
    "href": "docs/cards/project-card.html#continuous-improvement",
    "title": "Project Card",
    "section": "Continuous Improvement",
    "text": "Continuous Improvement\nHow will the system/model will improve? Provide a plan and means.\n\nUser Feedback Loop:\n\nChatbot Interactions: Track and analyze questions and responses from users (e.g., health workers or clinicians) to understand common inquiries, misconceptions, and desired information. This feedback will guide adjustments to the LLM’s responses and overall user experience.\nPrediction Accuracy: Collect feedback on predictions and confidence levels to assess areas of the model that need recalibration or fine-tuning, especially in cases that are challenging or prone to misclassification.\n\nData Collection and Expansion:\n\nCase Database Expansion: Gather additional dermatoscopic images from new rural cases, focusing on underrepresented lesion types or skin tones, to enhance model generalization.\nOut-of-Distribution (OOD) Tracking: Continuously monitor for OOD cases flagged by the model, then investigate and potentially incorporate these cases into training to improve robustness.\n\nScheduled Model Re-Training:\n\nAutomated Retraining Triggers: Implement retraining based on key metrics such as data drift, calibration error, and user feedback patterns to ensure that the model remains up-to-date with the latest data.\nData Augmentation: Periodically apply advanced data augmentation techniques (e.g., rotation, color adjustments) to create a more diverse and robust training dataset.\n\nModel Evaluation and Metrics Review:\n\nRegular Performance Audits: Conduct periodic evaluations to check model performance on critical metrics such as accuracy, false positive/negative rates, and OOD detection accuracy. Adjust hyperparameters and model architecture if necessary to address any consistent performance gaps.\nCalibration Checks: Regularly verify confidence scores to ensure accurate and reliable results, minimizing the risk of overconfidence in predictions."
  },
  {
    "objectID": "reference/preprocess_data_input.html",
    "href": "reference/preprocess_data_input.html",
    "title": "preprocess_data_input",
    "section": "",
    "text": "preprocess_data_input\npreprocess_data_input(train_data)\nPreprocesses the input training data for the DermaMNIST dataset.\nThis function applies transformations, including resizing images to 224x224 pixels and normalizing them with specified mean and standard deviation values.\nArgs: train_data (pd.DataFrame): The input training data in the form of a pandas DataFrame.\nReturns: DermaMNISTDataset: The preprocessed training dataset ready for model training."
  },
  {
    "objectID": "reference/normalizing_images.html",
    "href": "reference/normalizing_images.html",
    "title": "normalizing_images",
    "section": "",
    "text": "normalizing_images\nnormalizing_images(data)\nNormalizes the pixel values of images in the given DataFrame.\nThis function takes a DataFrame containing image data and normalizes the pixel values by dividing each pixel value by 255.0. The normalized pixel values will be in the range [0, 1].\nArgs: data (pd.DataFrame): A DataFrame containing image data. The DataFrame must have a column named “image” where each entry is an image represented as a numerical array.\nReturns: pd.DataFrame: A DataFrame with the same structure as the input, but with normalized image pixel values."
  },
  {
    "objectID": "reference/rmd_detector.html",
    "href": "reference/rmd_detector.html",
    "title": "rmd_detector",
    "section": "",
    "text": "rmd_detector\nrmd_detector(best_model_uri, in_dataset, out_dataset, batch_size, device)\nRun the Relative Mahalanobis Distance (RMD) detector on the given datasets.\nArgs: best_model_uri: The URI of the best model. in_dataset: The in-distribution dataset. out_dataset: The out-of-distribution dataset. batch_size: The batch size to use. device: The device to use.\nReturns: The metrics of the detector and the detector itself."
  },
  {
    "objectID": "reference/prepare_data_for_ood.html",
    "href": "reference/prepare_data_for_ood.html",
    "title": "prepare_data_for_ood",
    "section": "",
    "text": "prepare_data_for_ood\nprepare_data_for_ood(img)\nPrepare image for OOD detection. Args: img: Image to be prepared. Returns: Image prepared for OOD detection"
  },
  {
    "objectID": "reference/predict.html",
    "href": "reference/predict.html",
    "title": "predict",
    "section": "",
    "text": "predict\npredict(best_model, input_img, device)\nPredicts the output of the input image using the best model. Args: best_model: The best model to use for prediction. input_img: Input image to predict. device: Device to run the model on. Returns: output: The output tensor."
  },
  {
    "objectID": "reference/ood_detection.html",
    "href": "reference/ood_detection.html",
    "title": "ood_detection",
    "section": "",
    "text": "ood_detection\nood_detection(img, detector, threshold)\nDetect out-of-distribution samples. Args: img: Image to be detected. detector: MultiMahalanobis OOD detector. Returns: OOD score."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the data_preprocessing pipeline\n\n\n\nnormalizing_images\nNormalizes the pixel values of images in the given DataFrame.\n\n\ntensoring_resizing\nApplies a series of transformations to the ‘image’ column of a pandas DataFrame.\n\n\n\n\n\n\nFunctions in the model_training pipeline\n\n\n\nDermaMNISTDataset\nA custom Dataset class for the DermaMNIST dataset.\n\n\npreprocess_data_input\nPreprocesses the input training data for the DermaMNIST dataset.\n\n\nmodel_select\nSelects and returns a pre-trained model based on the provided model name.\n\n\nmodel_finetune\nFinetunes a pre-trained model on the given training dataset.\n\n\nevaluate_model\nEvaluates a pre-trained model on the given test dataset.\n\n\nlog_model\nLogs the model, hyperparameters, and metrics to MLFlow.\n\n\n\n\n\n\nFunctions in the ood_detection pipeline\n\n\n\nprepare_data\nPrepare the data for the OOD detection pipeline.\n\n\ntrain_wide_resnet\nTrain a WideResNet model on the given datasets.\n\n\nmulti_mahalanobis_detector\nRun the Multi-Mahalanobis detector on the given datasets.\n\n\n\n\n\n\nFunctions in the conformal_prediction pipeline\n\n\n\ndata_prep\nPrepare the calibration and test sets for the conformal prediction pipeline.\n\n\ncalibrate_predictor\nCalibrate a SplitPredictor using the calibration set.\n\n\nevaluate_predictor\nEvaluate a SplitPredictor using the test set.\n\n\n\n\n\n\nFunctions used in various inference pipelines\n\n\n\nresize_image\nResize image.\n\n\nnormalize_image\nNormalize image.\n\n\nprepare_data_for_ood\nPrepare image for OOD detection.\n\n\nood_detection\nDetect out-of-distribution samples.\n\n\npredict\nPredicts the output of the input image using the best model.\n\n\nconformal_prediction\nPerform conformal prediction on the output tensor.\n\n\nintegrated_gradients\nPerform integrated gradients on the input image.\n\n\nlog_prediction\nLog the prediction."
  },
  {
    "objectID": "reference/index.html#data_preprocessing",
    "href": "reference/index.html#data_preprocessing",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the data_preprocessing pipeline\n\n\n\nnormalizing_images\nNormalizes the pixel values of images in the given DataFrame.\n\n\ntensoring_resizing\nApplies a series of transformations to the ‘image’ column of a pandas DataFrame."
  },
  {
    "objectID": "reference/index.html#model_training",
    "href": "reference/index.html#model_training",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the model_training pipeline\n\n\n\nDermaMNISTDataset\nA custom Dataset class for the DermaMNIST dataset.\n\n\npreprocess_data_input\nPreprocesses the input training data for the DermaMNIST dataset.\n\n\nmodel_select\nSelects and returns a pre-trained model based on the provided model name.\n\n\nmodel_finetune\nFinetunes a pre-trained model on the given training dataset.\n\n\nevaluate_model\nEvaluates a pre-trained model on the given test dataset.\n\n\nlog_model\nLogs the model, hyperparameters, and metrics to MLFlow."
  },
  {
    "objectID": "reference/index.html#ood_detection",
    "href": "reference/index.html#ood_detection",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the ood_detection pipeline\n\n\n\nprepare_data\nPrepare the data for the OOD detection pipeline.\n\n\ntrain_wide_resnet\nTrain a WideResNet model on the given datasets.\n\n\nmulti_mahalanobis_detector\nRun the Multi-Mahalanobis detector on the given datasets."
  },
  {
    "objectID": "reference/index.html#conformal_prediction",
    "href": "reference/index.html#conformal_prediction",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the conformal_prediction pipeline\n\n\n\ndata_prep\nPrepare the calibration and test sets for the conformal prediction pipeline.\n\n\ncalibrate_predictor\nCalibrate a SplitPredictor using the calibration set.\n\n\nevaluate_predictor\nEvaluate a SplitPredictor using the test set."
  },
  {
    "objectID": "reference/index.html#inference",
    "href": "reference/index.html#inference",
    "title": "API Reference",
    "section": "",
    "text": "Functions used in various inference pipelines\n\n\n\nresize_image\nResize image.\n\n\nnormalize_image\nNormalize image.\n\n\nprepare_data_for_ood\nPrepare image for OOD detection.\n\n\nood_detection\nDetect out-of-distribution samples.\n\n\npredict\nPredicts the output of the input image using the best model.\n\n\nconformal_prediction\nPerform conformal prediction on the output tensor.\n\n\nintegrated_gradients\nPerform integrated gradients on the input image.\n\n\nlog_prediction\nLog the prediction."
  },
  {
    "objectID": "reference/log_model.html",
    "href": "reference/log_model.html",
    "title": "log_model",
    "section": "",
    "text": "log_model\nlog_model(model_name, model_state_dict, hyperparams, metrics, loss_plot)\nLogs the model, hyperparameters, and metrics to MLFlow.\nArgs: model_name (str): The name of the model. model_state_dict (dict): The state dictionary of the model. hyperparams (dict): The hyperparameters used during training. metrics (dict): The evaluation metrics of the model. loss_plot (plt.Figure): The plot of the training loss.\nReturns: str: The URI of the logged model."
  },
  {
    "objectID": "reference/calibrate_predictor.html",
    "href": "reference/calibrate_predictor.html",
    "title": "calibrate_predictor",
    "section": "",
    "text": "calibrate_predictor\ncalibrate_predictor(calibration_set, best_model_uri, alpha, penalty, batch_size)\nCalibrate a SplitPredictor using the calibration set.\nArgs: calibration_set: The calibration set. best_model_uri: The URI of the best model. alpha: The significance level. penalty: The penalty parameter for the RAPS score. batch_size: The batch size.\nReturns: The calibrated SplitPredict"
  },
  {
    "objectID": "reference/normalize_image.html",
    "href": "reference/normalize_image.html",
    "title": "normalize_image",
    "section": "",
    "text": "normalize_image\nnormalize_image(img, mean, std)\nNormalize image. Args: img: Image to be normalized. mean: Mean value for normalization. std: Standard deviation for normalization. Returns: Normalized image."
  },
  {
    "objectID": "reference/multi_mahalanobis_detector.html",
    "href": "reference/multi_mahalanobis_detector.html",
    "title": "multi_mahalanobis_detector",
    "section": "",
    "text": "multi_mahalanobis_detector\nmulti_mahalanobis_detector(\n    wide_resnet,\n    train_in_dataset,\n    test_in_dataset,\n    out_dataset,\n    batch_size,\n)\nRun the Multi-Mahalanobis detector on the given datasets.\nArgs: wide_resnet: The WideResNet model. train_in_dataset: The in-distribution training dataset. test_in_dataset: The in-distribution test dataset. out_dataset: The out-of-distribution dataset. batch_size: The batch size to use.\nReturns: The metrics of the detector and the detector itself."
  },
  {
    "objectID": "reference/evaluate_model.html",
    "href": "reference/evaluate_model.html",
    "title": "evaluate_model",
    "section": "",
    "text": "evaluate_model\nevaluate_model(\n    model_name,\n    model_state_dict,\n    test_dataset,\n    batch_size,\n    device='cpu',\n)\nEvaluates a pre-trained model on the given test dataset.\nArgs: model_name (str): The name of the model being evaluated. model_state_dict (dict): The state dictionary of the pre-trained model. test_dataset (DermaMNISTDataset): The dataset to evaluate the model on. batch_size (int): The batch size to use during evaluation. device (str): The device to evaluate the model on (e.g., “cpu” or “cuda”).\nReturns: dict: The classification report of the model’s performance."
  },
  {
    "objectID": "rough.html",
    "href": "rough.html",
    "title": "OncoDerm AI",
    "section": "",
    "text": "%load_ext kedro.ipython\n\n[11/18/24 23:55:03] INFO     Using 'conf/logging.yml' as logging configuration. You can change this __init__.py:270\n                             by setting the KEDRO_LOGGING_CONFIG environment variable accordingly.                 \n\n\n\n                    INFO     Registered line magic '%reload_kedro'                                   __init__.py:61\n\n\n\n                    INFO     Registered line magic '%load_node'                                      __init__.py:63\n\n\n\n                    INFO     Resolved project path as: /home/saimadhavang/sem7/mlpe/onco-derm-ai.   __init__.py:178\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[11/18/24 23:55:07] WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro_viz/__init__.py:13: KedroVizPythonVersionWarning:                      \n                             Please be advised that Kedro Viz is not yet fully                                     \n                                     compatible with the Python version you are currently using.                   \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/framework/project/__init__.py:347: UserWarning: The                    \n                             'onco_derm_ai.pipelines.model_inference' module does not expose a                     \n                             'create_pipeline' function, so no pipelines defined therein will be                   \n                             returned by 'find_pipelines'.                                                         \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/framework/project/__init__.py:347: UserWarning: The                    \n                             'onco_derm_ai.pipelines.ood_detection' module does not expose a                       \n                             'create_pipeline' function, so no pipelines defined therein will be                   \n                             returned by 'find_pipelines'.                                                         \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/framework/project/__init__.py:347: UserWarning: The                    \n                             'onco_derm_ai.pipelines.inf_data_preprocessing' module does not expose                \n                             a 'create_pipeline' function, so no pipelines defined therein will be                 \n                             returned by 'find_pipelines'.                                                         \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/18/24 23:55:08] INFO     Kedro project onco-derm-ai                                             __init__.py:144\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:145\n                             'pipelines'                                                                           \n\n\n\n[11/18/24 23:55:09] INFO     Registered line magic 'run_viz'                                        __init__.py:151\n\n\n\n\ntrain_raw = catalog.load(\"train_raw\")\n\n[11/18/24 23:55:16] INFO     Loading data from train_raw (PickleDataset)...                     data_catalog.py:389\n\n\n\n\nimport numpy as np\n\n\nnp.stack(train_raw['image'])\n\n\n\n\n\narray([[[[158, 111, 117],\n         [161, 116, 121],\n         [164, 121, 130],\n         ...,\n         [189, 160, 164],\n         [187, 158, 160],\n         [186, 157, 159]],\n        [[160, 113, 119],\n         [165, 120, 125],\n         [170, 129, 137],\n         ...,\n         [191, 162, 166],\n         [189, 160, 162],\n         [188, 159, 161]],\n        [[164, 119, 124],\n         [170, 128, 132],\n         [177, 137, 145],\n         ...,\n         [191, 162, 166],\n         [188, 162, 165],\n         [187, 161, 164]],\n        ...,\n        [[172, 142, 150],\n         [176, 147, 152],\n         [182, 150, 155],\n         ...,\n         [195, 167, 164],\n         [181, 153, 150],\n         [171, 143, 140]],\n        [[176, 147, 152],\n         [176, 147, 152],\n         [180, 148, 153],\n         ...,\n         [188, 160, 157],\n         [178, 150, 147],\n         [171, 143, 140]],\n        [[175, 146, 151],\n         [173, 144, 149],\n         [174, 142, 147],\n         ...,\n         [180, 152, 149],\n         [175, 147, 144],\n         [173, 145, 142]]],\n       [[[230, 111, 105],\n         [226, 107,  99],\n         [225, 106,  98],\n         ...,\n         [217, 113, 102],\n         [215, 111, 100],\n         [213, 109,  98]],\n        [[225, 106, 100],\n         [221, 102,  94],\n         [220, 102,  92],\n         ...,\n         [218, 114, 103],\n         [216, 112, 101],\n         [215, 111, 100]],\n        [[222, 103,  97],\n         [218,  99,  91],\n         [217, 100,  90],\n         ...,\n         [219, 115, 104],\n         [217, 115, 103],\n         [216, 114, 102]],\n        ...,\n        [[212, 119, 102],\n         [214, 121, 104],\n         [215, 122, 104],\n         ...,\n         [213, 121, 100],\n         [212, 122,  98],\n         [211, 121,  97]],\n        [[212, 119, 101],\n         [214, 121, 103],\n         [217, 122, 104],\n         ...,\n         [213, 122, 101],\n         [212, 122,  98],\n         [211, 121,  97]],\n        [[212, 119, 101],\n         [214, 121, 103],\n         [217, 122, 104],\n         ...,\n         [213, 122, 101],\n         [212, 122,  98],\n         [211, 121,  97]]],\n       [[[229, 156, 173],\n         [229, 156, 173],\n         [227, 156, 174],\n         ...,\n         [236, 174, 177],\n         [239, 172, 179],\n         [238, 171, 178]],\n        [[231, 158, 175],\n         [230, 157, 174],\n         [228, 157, 175],\n         ...,\n         [233, 171, 174],\n         [235, 170, 176],\n         [235, 168, 175]],\n        [[231, 160, 176],\n         [230, 159, 175],\n         [229, 158, 174],\n         ...,\n         [236, 174, 179],\n         [237, 172, 178],\n         [237, 172, 178]],\n        ...,\n        [[195, 147, 159],\n         [197, 149, 161],\n         [200, 152, 164],\n         ...,\n         [218, 161, 170],\n         [217, 160, 169],\n         [217, 160, 169]],\n        [[196, 149, 159],\n         [197, 150, 160],\n         [198, 151, 161],\n         ...,\n         [216, 160, 169],\n         [216, 160, 169],\n         [216, 160, 169]],\n        [[197, 150, 158],\n         [197, 150, 158],\n         [197, 150, 158],\n         ...,\n         [216, 160, 169],\n         [216, 160, 169],\n         [215, 159, 168]]],\n       ...,\n       [[[ 20,  18,  32],\n         [ 54,  52,  66],\n         [ 95,  94, 108],\n         ...,\n         [ 96,  98, 110],\n         [ 48,  51,  60],\n         [ 16,  19,  28]],\n        [[ 49,  47,  61],\n         [ 78,  76,  90],\n         [114, 112, 126],\n         ...,\n         [118, 120, 132],\n         [ 86,  86,  98],\n         [ 58,  61,  70]],\n        [[ 89,  85,  99],\n         [110, 106, 120],\n         [136, 132, 147],\n         ...,\n         [141, 141, 153],\n         [125, 123, 136],\n         [106, 106, 118]],\n        ...,\n        [[122, 116, 130],\n         [137, 133, 147],\n         [151, 148, 159],\n         ...,\n         [149, 136, 145],\n         [133, 120, 129],\n         [114, 101, 110]],\n        [[ 73,  65,  78],\n         [108, 102, 114],\n         [144, 138, 150],\n         ...,\n         [137, 124, 133],\n         [114, 101, 110],\n         [ 90,  77,  86]],\n        [[ 29,  21,  34],\n         [ 77,  69,  82],\n         [127, 121, 133],\n         ...,\n         [123, 110, 119],\n         [ 94,  81,  90],\n         [ 65,  52,  61]]],\n       [[[171, 141, 177],\n         [172, 142, 178],\n         [175, 143, 180],\n         ...,\n         [169, 129, 156],\n         [169, 130, 157],\n         [169, 130, 157]],\n        [[175, 145, 181],\n         [176, 146, 180],\n         [178, 146, 183],\n         ...,\n         [175, 135, 162],\n         [174, 135, 162],\n         [174, 135, 162]],\n        [[182, 151, 185],\n         [181, 150, 182],\n         [182, 151, 185],\n         ...,\n         [180, 140, 167],\n         [179, 139, 166],\n         [177, 137, 164]],\n        ...,\n        [[171, 128, 158],\n         [171, 128, 158],\n         [170, 126, 159],\n         ...,\n         [173, 142, 176],\n         [171, 140, 174],\n         [169, 138, 172]],\n        [[177, 136, 166],\n         [173, 132, 162],\n         [168, 127, 159],\n         ...,\n         [169, 138, 172],\n         [168, 134, 169],\n         [166, 132, 167]],\n        [[179, 140, 169],\n         [174, 135, 164],\n         [168, 127, 159],\n         ...,\n         [169, 138, 172],\n         [169, 135, 170],\n         [167, 133, 168]]],\n       [[[239, 156, 176],\n         [235, 152, 172],\n         [231, 149, 171],\n         ...,\n         [241, 160, 179],\n         [237, 158, 177],\n         [236, 157, 176]],\n        [[236, 153, 173],\n         [234, 151, 171],\n         [232, 149, 169],\n         ...,\n         [241, 160, 179],\n         [238, 159, 178],\n         [237, 158, 177]],\n        [[235, 150, 169],\n         [235, 150, 169],\n         [235, 150, 169],\n         ...,\n         [239, 160, 179],\n         [238, 159, 178],\n         [237, 158, 177]],\n        ...,\n        [[231, 168, 179],\n         [232, 166, 178],\n         [230, 162, 173],\n         ...,\n         [226, 154, 168],\n         [227, 155, 169],\n         [228, 156, 170]],\n        [[232, 166, 178],\n         [232, 166, 178],\n         [231, 163, 174],\n         ...,\n         [227, 156, 172],\n         [225, 154, 170],\n         [224, 153, 169]],\n        [[232, 166, 178],\n         [231, 165, 177],\n         [233, 163, 174],\n         ...,\n         [228, 157, 173],\n         [225, 154, 170],\n         [222, 151, 167]]]], dtype=uint8)\n\n\n\n\ntrain_raw['label'].to_numpy(dtype=np.int32)\n\n\n\n\n\narray([0, 5, 5, ..., 2, 5, 5], dtype=int32)\n\n\n\n\ndetector = catalog.load(\"ood_detector\")\n\n[11/18/24 22:37:46] INFO     Loading data from ood_detector (PickleDataset)...                  data_catalog.py:389\n\n\n\n\ndetector\n\n\n\n\n\n&lt;pytorch_ood.detector.mmahalanobis.MultiMahalanobis object at 0x7fecf60c82c0&gt;\n\n\n\n\nsample_img = catalog.load(\"inference_sample\")\n\n[11/18/24 15:46:37] INFO     Loading data from inference_sample (ImageDataset)...               data_catalog.py:389\n\n\n\n\n%load_node resize_image_node_inference\n\n[11/18/24 15:48:23] WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/ipython/__init__.py:319: UserWarning: This is an                       \n                             experimental feature, only Jupyter Notebook (&gt;7.0), Jupyter Lab,                      \n                             IPython, and VSCode Notebook are supported. If you encounter                          \n                             unexpected behaviour or would like to suggest feature enhancements,                   \n                             add it under this github issue                                                        \n                             https://github.com/kedro-org/kedro/issues/3580                                        \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Loading node definition from                                           __init__.py:360\n                             /home/saimadhavang/sem7/mlpe/onco-derm-ai/src/onco_derm_ai/pipelines/i                \n                             nf_data_preprocessing/nodes.py                                                        \n\n\n\n\n# Prepare necessary inputs for debugging\n# All debugging inputs must be defined in your project catalog\nimg = catalog.load(\"inference_sample\")\nsize = catalog.load(\"params:img_size\")\n\nfrom typing import Tuple\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F\nfrom PIL import Image\n\ndef resize_image(img: Image, size: Tuple[int, int]) -&gt; torch.Tensor:\n    \"\"\"\n    Resize image.\n    Args:\n        img: Image to be resized.\n        size: New size for the image.\n    Returns:\n        Resized image.\n    \"\"\"\n    img = np.array(img.convert(\"RGB\")).astype(np.float32)\n    img = F.to_tensor(img)/255.0\n    img = F.resize(img, size)\n    return img\n\n\nresized_img = resize_image(img, size)\n\n[11/18/24 15:55:53] INFO     Loading data from inference_sample (ImageDataset)...               data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:img_size (MemoryDataset)...               data_catalog.py:389\n\n\n\n\n%load_node normalize_image_node_inference\n\n[11/18/24 15:48:34] WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/ipython/__init__.py:319: UserWarning: This is an                       \n                             experimental feature, only Jupyter Notebook (&gt;7.0), Jupyter Lab,                      \n                             IPython, and VSCode Notebook are supported. If you encounter                          \n                             unexpected behaviour or would like to suggest feature enhancements,                   \n                             add it under this github issue                                                        \n                             https://github.com/kedro-org/kedro/issues/3580                                        \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Loading node definition from                                           __init__.py:360\n                             /home/saimadhavang/sem7/mlpe/onco-derm-ai/src/onco_derm_ai/pipelines/i                \n                             nf_data_preprocessing/nodes.py                                                        \n\n\n\n\n# Prepare necessary inputs for debugging\n# All debugging inputs must be defined in your project catalog\nmean = catalog.load(\"params:normal_mean\")\nstd = catalog.load(\"params:normal_std\")\n\nfrom typing import Tuple\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F\nfrom PIL import Image\n\ndef normalize_image(\n    img: torch.Tensor, mean: Tuple[float, float, float], std: Tuple[float, float, float]\n) -&gt; torch.tensor:\n    \"\"\"\n    Normalize image.\n    Args:\n        img: Image to be normalized.\n        mean: Mean value for normalization.\n        std: Standard deviation for normalization.\n    Returns:\n        Normalized image.\n    \"\"\"\n    img = F.normalize(img, mean, std)\n    return img\n\n\noutput = normalize_image(resized_img, mean, std)\n\n[11/18/24 15:55:56] INFO     Loading data from params:normal_mean (MemoryDataset)...            data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_std (MemoryDataset)...             data_catalog.py:389\n\n\n\n\nresized_img.shape\n\n\n\n\n\ntorch.Size([3, 224, 224])\n\n\n\n\nresized_img \n\n\n\n\n\ntensor([[[1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9490, 0.9490],\n         [1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9490, 0.9490],\n         [1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9490, 0.9490],\n         ...,\n         [0.9373, 0.9373, 0.9373,  ..., 0.9020, 0.9020, 0.9020],\n         [0.9373, 0.9373, 0.9373,  ..., 0.9020, 0.9020, 0.9020],\n         [0.9373, 0.9373, 0.9373,  ..., 0.9020, 0.9020, 0.9020]],\n        [[0.6745, 0.6745, 0.6745,  ..., 0.6549, 0.6549, 0.6549],\n         [0.6745, 0.6745, 0.6745,  ..., 0.6549, 0.6549, 0.6549],\n         [0.6745, 0.6745, 0.6745,  ..., 0.6549, 0.6549, 0.6549],\n         ...,\n         [0.6353, 0.6353, 0.6353,  ..., 0.6157, 0.6157, 0.6157],\n         [0.6353, 0.6353, 0.6353,  ..., 0.6157, 0.6157, 0.6157],\n         [0.6353, 0.6353, 0.6353,  ..., 0.6157, 0.6157, 0.6157]],\n        [[0.7020, 0.7020, 0.7020,  ..., 0.6824, 0.6824, 0.6824],\n         [0.7020, 0.7020, 0.7020,  ..., 0.6824, 0.6824, 0.6824],\n         [0.7020, 0.7020, 0.7020,  ..., 0.6824, 0.6824, 0.6824],\n         ...,\n         [0.6039, 0.6039, 0.6039,  ..., 0.5922, 0.5922, 0.5922],\n         [0.6039, 0.6039, 0.6039,  ..., 0.5922, 0.5922, 0.5922],\n         [0.6039, 0.6039, 0.6039,  ..., 0.5922, 0.5922, 0.5922]]])\n\n\n\n\noutput\n\n\n\n\n\ntensor([[[2.2489, 2.2489, 2.2489,  ..., 2.0263, 2.0263, 2.0263],\n         [2.2489, 2.2489, 2.2489,  ..., 2.0263, 2.0263, 2.0263],\n         [2.2489, 2.2489, 2.2489,  ..., 2.0263, 2.0263, 2.0263],\n         ...,\n         [1.9749, 1.9749, 1.9749,  ..., 1.8208, 1.8208, 1.8208],\n         [1.9749, 1.9749, 1.9749,  ..., 1.8208, 1.8208, 1.8208],\n         [1.9749, 1.9749, 1.9749,  ..., 1.8208, 1.8208, 1.8208]],\n        [[0.9755, 0.9755, 0.9755,  ..., 0.8880, 0.8880, 0.8880],\n         [0.9755, 0.9755, 0.9755,  ..., 0.8880, 0.8880, 0.8880],\n         [0.9755, 0.9755, 0.9755,  ..., 0.8880, 0.8880, 0.8880],\n         ...,\n         [0.8004, 0.8004, 0.8004,  ..., 0.7129, 0.7129, 0.7129],\n         [0.8004, 0.8004, 0.8004,  ..., 0.7129, 0.7129, 0.7129],\n         [0.8004, 0.8004, 0.8004,  ..., 0.7129, 0.7129, 0.7129]],\n        [[1.3154, 1.3154, 1.3154,  ..., 1.2282, 1.2282, 1.2282],\n         [1.3154, 1.3154, 1.3154,  ..., 1.2282, 1.2282, 1.2282],\n         [1.3154, 1.3154, 1.3154,  ..., 1.2282, 1.2282, 1.2282],\n         ...,\n         [0.8797, 0.8797, 0.8797,  ..., 0.8274, 0.8274, 0.8274],\n         [0.8797, 0.8797, 0.8797,  ..., 0.8274, 0.8274, 0.8274],\n         [0.8797, 0.8797, 0.8797,  ..., 0.8274, 0.8274, 0.8274]]])\n\n\n\n\nimport mlflow\n\nmlflow.set_tracking_uri('http://localhost:5000')\nmodel = mlflow.pytorch.load_model(catalog.load(\"best_model_uri\"))\n\n[11/18/24 16:04:40] INFO     Loading data from best_model_uri (TextDataset)...                  data_catalog.py:389\n\n\n\n\n\n\n\nmodel(output.unsqueeze(0))\n\n\n\n\n\ntensor([[-6.9457, -7.5042, -5.5099, -6.1375, -3.4725, 10.2126, -4.6117]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\ndetector(output.unsqueeze(0).to(\"cuda\"))\n\n\n\n\n\ntensor([-2.9780e-05], device='cuda:0', grad_fn=&lt;MinBackward0&gt;)\n\n\n\n\noutput.device\n\n\n\n\n\ndevice(type='cpu')\n\n\n\n\nfrom torchvision.datasets import CIFAR10\n\n\ncifar10 = CIFAR10(root=\"~/.data\", download=True)\n\nFiles already downloaded and verified\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimg, label = cifar10[5126]\nplt.imshow(np.array(img))\n\n\n\n\n\n&lt;matplotlib.image.AxesImage object at 0x7f4d381fbd10&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndetector(normalize_image(resize_image(img, size), mean, std).unsqueeze(0).to(\"cuda\"))\n\n\n\n\n\ntensor([0.0001], device='cuda:0', grad_fn=&lt;MinBackward0&gt;)\n\n\n\n\n%reload_kedro\n\n[11/18/24 22:36:36] INFO     Resolved project path as: /home/saimadhavang/sem7/mlpe/onco-derm-ai.   __init__.py:178\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n                    INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/18/24 22:36:37] INFO     Kedro project onco-derm-ai                                             __init__.py:144\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:145\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:151\n\n\n\n\n%load_node ood_prepare_data\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/ipython/__init__.py:319: UserWarning: This is an                       \n                             experimental feature, only Jupyter Notebook (&gt;7.0), Jupyter Lab,                      \n                             IPython, and VSCode Notebook are supported. If you encounter                          \n                             unexpected behaviour or would like to suggest feature enhancements,                   \n                             add it under this github issue                                                        \n                             https://github.com/kedro-org/kedro/issues/3580                                        \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Loading node definition from                                           __init__.py:360\n                             /home/saimadhavang/sem7/mlpe/onco-derm-ai/src/onco_derm_ai/pipelines/o                \n                             od_detection/nodes.py                                                                 \n\n\n\n\n# Prepare necessary inputs for debugging\n# All debugging inputs must be defined in your project catalog\nout_ds = catalog.load(\"params:ood_detection_out_ds\")\nsize = catalog.load(\"params:img_size\")\nnormal_mean = catalog.load(\"params:normal_mean\")\nnormal_std = catalog.load(\"params:normal_std\")\n\nfrom typing import Tuple\nimport medmnist\nimport mlflow\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom medmnist import INFO\nfrom pytorch_ood.detector import RMD, MaxSoftmax, MultiMahalanobis\nfrom pytorch_ood.model import WideResNet\nfrom pytorch_ood.utils import OODMetrics, ToUnknown\nfrom torch.utils.data import ConcatDataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom tqdm import tqdm\n\ndef prepare_data(\n    out_ds: str,\n    size: Tuple[int, int],\n    normal_mean: Tuple[float, float, float],\n    normal_std: Tuple[float, float, float],\n) -&gt; Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    \"\"\"\n    Prepare the data for the OOD detection pipeline.\n\n    Args:\n        out_ds: The dataset to use as the OOD dataset.\n        size: The size to resize the images to.\n        normal_mean: The mean to normalize the images.\n        normal_std: The standard deviation to normalize the images.\n\n    Returns:\n        The in-distribution dataset and the out-of-distribution dataset.\n    \"\"\"\n    # transform = transforms.Compose(\n    #     [\n    #         transforms.Resize(size),\n    #         transforms.ToTensor(),\n    #         transforms.Normalize(normal_mean, normal_std),\n    #     ]\n    # )\n\n    transform = WideResNet.transform_for(\"cifar10-pt\")\n\n    info = INFO[\"dermamnist\"]\n    DataClass = getattr(medmnist, info[\"python_class\"])\n    dataset_in_train = DataClass(split=\"train\", download=True, transform=transform)\n    dataset_in_val = DataClass(split=\"val\", download=True, transform=transform)\n    dataset_in_test = DataClass(split=\"test\", download=True, transform=transform)\n    if out_ds == \"cifar10\":\n        dataset_out_test = CIFAR10(\n            root=\"~/.data\",\n            download=True,\n            transform=transform,\n            target_transform=ToUnknown(),\n        )\n    else:\n        raise ValueError(\"Invalid out_ds\")\n\n    in_dataset_test = ConcatDataset([dataset_in_val, dataset_in_test])\n\n    return dataset_in_train, in_dataset_test, dataset_out_test\n\n\nin_train, in__test, out_ds = prepare_data(out_ds, size, normal_mean, normal_std)\n\n[11/18/24 22:37:09] INFO     Loading data from params:ood_detection_out_ds (MemoryDataset)...   data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:img_size (MemoryDataset)...               data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_mean (MemoryDataset)...            data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_std (MemoryDataset)...             data_catalog.py:389\n\n\n\nUsing downloaded and verified file: /home/saimadhavang/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/saimadhavang/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/saimadhavang/.medmnist/dermamnist.npz\nFiles already downloaded and verified\n\n\n\nin_scores = []\nout_scores = []\n\nfrom torch.utils.data import DataLoader, ConcatDataset\n\nin_ds = ConcatDataset([in_train, in__test])\n\n\nin_loader = DataLoader(in_ds, batch_size=128, shuffle=True)\nout_loader = DataLoader(out_ds, batch_size=128, shuffle=True)\n\n\nfor batch in tqdm(in_loader):\n    in_scores.extend(detector(batch[0].to(\"cuda\")).detach().cpu().numpy())\n\n100%|██████████| 79/79 [00:05&lt;00:00, 13.85it/s]\n\n\n\nfor batch in tqdm(out_loader):\n    out_scores.extend(detector(batch[0].to(\"cuda\")).detach().cpu().numpy())\n\n100%|██████████| 391/391 [00:24&lt;00:00, 16.10it/s]\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.hist(in_scores, bins=500, alpha=0.5, label=\"In-distribution\")\nplt.hist(out_scores, bins=1000, alpha=0.5, label=\"Out-of-distribution\")\nplt.xlim(-0.1, 0.1)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nin_scores = np.array(in_scores)\nout_scores = np.array(out_scores)\n\n\nthreshold = 0.07\n(in_scores &gt; threshold).sum() / len(in_scores), (out_scores &gt; threshold).sum() / len(out_scores)\n\n\n\n\n\n(np.float64(0.01727408886669995), np.float64(0.98474))"
  },
  {
    "objectID": "reference/tensoring_resizing.html",
    "href": "reference/tensoring_resizing.html",
    "title": "tensoring_resizing",
    "section": "",
    "text": "tensoring_resizing\ntensoring_resizing(data)\nApplies a series of transformations to the ‘image’ column of a pandas DataFrame.\nThe transformations include converting images to PIL format, resizing them to 28x28 pixels, and converting them to tensors. The transformed images are then permuted and converted back to numpy arrays.\nArgs: data (pd.DataFrame): A pandas DataFrame containing an ‘image’ column with image data.\nReturns: pd.DataFrame: The input DataFrame with the ‘image’ column transformed."
  },
  {
    "objectID": "reference/evaluate_predictor.html",
    "href": "reference/evaluate_predictor.html",
    "title": "evaluate_predictor",
    "section": "",
    "text": "evaluate_predictor\nevaluate_predictor(predictor, test_set, batch_size)\nEvaluate a SplitPredictor using the test set.\nArgs: predictor: The SplitPredictor to evaluate. test_set: The test set. batch_size: The batch size.\nReturns: A dictionary containing the metrics."
  },
  {
    "objectID": "reference/conformal_prediction.html",
    "href": "reference/conformal_prediction.html",
    "title": "conformal_prediction",
    "section": "",
    "text": "conformal_prediction\nconformal_prediction(output, predictor)\nPerform conformal prediction on the output tensor.\nArgs: output: The output tensor. predictor: The SplitPredictor.\nReturns: The prediction."
  },
  {
    "objectID": "reference/model_finetune.html",
    "href": "reference/model_finetune.html",
    "title": "model_finetune",
    "section": "",
    "text": "model_finetune\nmodel_finetune(\n    train_dataset,\n    val_dataset,\n    model_name,\n    train_params,\n    device='cpu',\n)\nFinetunes a pre-trained model on the given training dataset.\nArgs: train_dataset (DermaMNISTDataset): The dataset to train the model on. model_name (str): The name of the model to finetune. train_params (dict): A dictionary containing the training parameters. device (str): The device to train the model on (e.g., “cpu” or “cuda”).\nReturns: dict: The state dictionary of the finetuned model."
  },
  {
    "objectID": "reference/model_select.html",
    "href": "reference/model_select.html",
    "title": "model_select",
    "section": "",
    "text": "model_select\nmodel_select(model_name, num_outputs=7, pretrained=False)\nSelects and returns a pre-trained model based on the provided model name.\nArgs: model_name (str): The name of the model to select. Supported values are “ResNet18” and “VGG16”. num_outputs (int): The number of output classes for the model. pretrained (bool): If True, loads a pre-trained model.\nReturns: models: The selected pre-trained model.\nRaises: ValueError: If the provided model name is not supported."
  },
  {
    "objectID": "reference/log_prediction.html",
    "href": "reference/log_prediction.html",
    "title": "log_prediction",
    "section": "",
    "text": "log_prediction\nlog_prediction(prediction)\nLog the prediction.\nArgs: prediction: The prediction.\nReturns: None"
  },
  {
    "objectID": "reference/data_prep.html",
    "href": "reference/data_prep.html",
    "title": "data_prep",
    "section": "",
    "text": "data_prep\ndata_prep(size, mean, std)\nPrepare the calibration and test sets for the conformal prediction pipeline.\nArgs: size: The size to which the images should be resized. mean: The mean values for normalization. std: The standard deviation values for normalization.\nReturns: A tuple containing the calibration and test sets."
  },
  {
    "objectID": "reference/resize_image.html",
    "href": "reference/resize_image.html",
    "title": "resize_image",
    "section": "",
    "text": "resize_image\nresize_image(img, size)\nResize image. Args: img: Image to be resized. size: New size for the image. Returns: Resized image."
  },
  {
    "objectID": "reference/prepare_data.html",
    "href": "reference/prepare_data.html",
    "title": "prepare_data",
    "section": "",
    "text": "prepare_data\nprepare_data(out_ds)\nPrepare the data for the OOD detection pipeline.\nArgs: out_ds: The out-of-distribution dataset to use.\nReturns: The in-distribution dataset and the out-of-distribution dataset."
  },
  {
    "objectID": "reference/DermaMNISTDataset.html",
    "href": "reference/DermaMNISTDataset.html",
    "title": "DermaMNISTDataset",
    "section": "",
    "text": "DermaMNISTDataset\nDermaMNISTDataset(self, dataframe, transform=None)\nA custom Dataset class for the DermaMNIST dataset.\nArgs: dataframe (pd.DataFrame): A pandas DataFrame containing the dataset. It should have two columns: ‘image’ and ‘label’. ‘image’ should contain (28, 28, 3) numpy arrays. transform (callable, optional): Optional transform to be applied on a sample.\nAttributes: dataframe (pd.DataFrame): The dataframe containing the dataset. transform (callable): The transform to be applied on a sample.\nMethods: len(): Returns the length of the dataset. getitem(idx): Returns the image and label at the given index."
  },
  {
    "objectID": "reference/integrated_gradients.html",
    "href": "reference/integrated_gradients.html",
    "title": "integrated_gradients",
    "section": "",
    "text": "integrated_gradients\nintegrated_gradients(\n    best_model,\n    input_processed_img,\n    input_img,\n    predictions,\n    show=False,\n)\nPerform integrated gradients on the input image.\nArgs: best_model: The best model. input_processed_img: The processed input image. input_img: The input image. predictions: The predictions. show: Whether to show the figures.\nReturns: The visualizations."
  },
  {
    "objectID": "reference/train_wide_resnet.html",
    "href": "reference/train_wide_resnet.html",
    "title": "train_wide_resnet",
    "section": "",
    "text": "train_wide_resnet\ntrain_wide_resnet(in_dataset, test_dataset, n_epochs, batch_size, device)\nTrain a WideResNet model on the given datasets.\nArgs: in_dataset: The in-distribution dataset. test_dataset: The test dataset. n_epochs: The number of epochs to train for. batch_size: The batch size to use. device: The device to use.\nReturns: The trained model."
  },
  {
    "objectID": "docs/cards/index.html",
    "href": "docs/cards/index.html",
    "title": "Cards",
    "section": "",
    "text": "Cards are intended to serve as a document giving a high level overview of the system developed and deployed. It is meant to be a high level document and as details emerge, documents such Model Cards and Data Cards can be linked. Below are the cards relevant to the OncoDermAI project:\n\nProject Card\nData Card\nModel card"
  },
  {
    "objectID": "docs/cards/Model-card.html#model-details",
    "href": "docs/cards/Model-card.html#model-details",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Model Details",
    "text": "Model Details\n\nPerson or Organization Developing Model:\nThis implementation was developed by Sai Madhavan and M Srinivasan as part of a project for AI-839 for skin lesion classification using the DermaMNIST dataset.\nModel Date:\nNovember 2024\nModel Version:\nResNet-18\nModel Type:\nConvolutional Neural Network (CNN)\nInformation about Training Algorithms, Parameters, Fairness Constraints, or Other Applied Approaches, and Features:\nResNet-18 is a deep residual network that introduces skip connections to solve vanishing gradient problems in deep networks. It was trained using the following parameters:\n\nOptimizer: Adam\n\nLearning Rate: 0.001\n\nBatch Size: 32\n\nNumber of Epochs: 20\n\nLoss Function: Cross-Entropy Loss\n\nFine Tune: We used a pre-trained Resnet-18 model from Pytorch and then fine tuned it for classification purposes on the DermaMNIST dataset.\n\nModified the final layers to match the number of classes in the DermaMnist dataset.\nTrained the modified model on the DermaMnist dataset.\nEvaluated the performance of the fine-tuned model.\n\nPaper or Other Resource for More Information:\n\nResNet Original Paper (He et al., 2015)\n\nDermaMNIST Dataset Resource\n\nCitation Details:\n\nResNet: He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\nDermaMNIST: Yang, Jiancheng, et al. “MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis.” arXiv preprint arXiv:2110.14795 (2021).\n\nLicense:\nApache License 2.0 for the ResNet model implementation; refer to the MedMNIST dataset licensing for dataset usage.\n\n\nPlease reach out to M Srinivasan - (mail id) or Sai Madhavan - (mail id) for any questions or comments about the model."
  },
  {
    "objectID": "docs/cards/Model-card.html#intended-use",
    "href": "docs/cards/Model-card.html#intended-use",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Intended Use",
    "text": "Intended Use\n\nPrimary Intended Uses:\n\nEarly screening for skin cancer and other serious skin conditions in resource-constrained settings.\n\nAssisting non-specialist healthcare workers in making preliminary assessments of skin lesions.\n\nIdentifying high-risk cases that require referral to dermatology specialists for further evaluation.\n\nPrimary Intended Users:\n\nNon-specialist healthcare workers, such as general practitioners, nurses, and community health workers in rural India.\n\nHealthcare providers in underserved regions lacking access to specialized dermatological support.\n\nOut-of-Scope Use Cases:\n\nProviding a definitive medical diagnosis without specialist consultation.\n\nUse in advanced clinical settings where dermatologists and specialized diagnostic tools are available.\n\nScreening for skin conditions outside the scope of the model’s training (e.g., non-cancerous cosmetic dermatology issues)."
  },
  {
    "objectID": "docs/cards/Model-card.html#factors",
    "href": "docs/cards/Model-card.html#factors",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Factors",
    "text": "Factors\n\nRelevant Factors:\n\nDemographic and phenotypic groups, including variations in skin tone and lesion types.\n\nEnvironmental conditions like lighting in image capture.\n\nEvaluation Factors:\n\nClassification accuracy, precision, recall, and F1 score across demographic groups.\n\nSensitivity to image preprocessing and augmentation techniques."
  },
  {
    "objectID": "docs/cards/Model-card.html#metrics",
    "href": "docs/cards/Model-card.html#metrics",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Metrics",
    "text": "Metrics\n\nModel Performance Measures:\n\nAccuracy: Overall percentage of correctly classified samples.\n\nPrecision: Proportion of true positives among predicted positives.\n\nRecall (Sensitivity): Proportion of true positives among actual positives.\n\nF1 Score: Harmonic mean of precision and recall."
  },
  {
    "objectID": "docs/cards/Model-card.html#evaluation-data",
    "href": "docs/cards/Model-card.html#evaluation-data",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Evaluation Data",
    "text": "Evaluation Data\n\nDatasets:\nThe evaluation was conducted on a subset of the dataset split into training, validation, and test sets. Images were resized to (224, 224) and normalized using ImageNet mean ([0.485, 0.456, 0.406]) and standard deviation ([0.229, 0.224, 0.225]).\nMotivation:\nThe goal of the evaluation data preparation was to ensure consistent preprocessing and transformations for fair model assessment. This included resizing and normalization to align with ResNet-18’s pretrained parameters.\nPreprocessing:\n\nImages were normalized by dividing pixel values by 255.\n\nApplied a transforms.Compose pipeline to perform resizing and normalization.\n\nFor additional evaluation, data augmentation techniques like horizontal and vertical flips, rotation, and color jitter were applied during testing to assess model robustness."
  },
  {
    "objectID": "docs/cards/Model-card.html#training-data",
    "href": "docs/cards/Model-card.html#training-data",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Training Data",
    "text": "Training Data\n\nDatasets:\nTraining data consisted of the DermaMNIST dataset, augmented to increase diversity and robustness. Images were resized to (224, 224) and normalized similarly to the evaluation data.\nPreprocessing:\n\nImages were converted to tensors and resized to (224, 224).\n\nNormalization used the ImageNet mean and standard deviation.\n\nData augmentation techniques were extensively applied, including horizontal/vertical flips, random rotations, and color jittering.\n\nFor each original image, multiple augmented versions were generated and added to the dataset using the provided augmentation function, with unique identifiers for each augmented sample."
  },
  {
    "objectID": "docs/cards/Model-card.html#quantitative-analyses",
    "href": "docs/cards/Model-card.html#quantitative-analyses",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Quantitative Analyses",
    "text": "Quantitative Analyses\n\nUnitary Results:\n\nAccuracy: 75%\n\nMacro F1 Score: 0.539\n\nIntersectional Results:\n\nPerformance variation observed across lesion types, with lower recall for rarer lesion classes."
  },
  {
    "objectID": "docs/cards/Model-card.html#ethical-considerations",
    "href": "docs/cards/Model-card.html#ethical-considerations",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\nPotential bias in training data due to overrepresentation of certain demographics or lesion types.\n\nModel is not a substitute for clinical expertise and should be used only for research purposes.\n\nMisclassifications could lead to inappropriate confidence in model predictions."
  },
  {
    "objectID": "docs/cards/Model-card.html#caveats-and-recommendations",
    "href": "docs/cards/Model-card.html#caveats-and-recommendations",
    "title": "Model Card for ResNet-18 on DermaMNIST Image Classification",
    "section": "Caveats and Recommendations",
    "text": "Caveats and Recommendations\n\nThe model’s performance is dependent on the quality of input images and generalizes poorly to out-of-distribution samples.\n\nClinical use requires thorough validation and expert oversight.\n\nFuture work should focus on improving robustness to demographic and environmental variations."
  },
  {
    "objectID": "notebooks/KedroBootTutorial.html",
    "href": "notebooks/KedroBootTutorial.html",
    "title": "OncoDerm AI",
    "section": "",
    "text": "from kedro_boot.app.booter import boot_package\nfrom kedro_boot.framework.compiler.specs import CompilationSpec\n\n\nsession = boot_package(\n    package_name=\"onco_derm_ai\",\n    compilation_specs=[\n        CompilationSpec(\n            inputs=[\"inference_sample\"], outputs=[\"predictions\", \"integrated_gradients\"]\n        )\n    ],\n    kedro_args={\n        \"pipeline\": \"inference\",\n        \"conf_source\": \"../conf/\",\n    },\n)\n\n[11/24/24 14:17:56] INFO     Kedro project onco-derm-ai                                              session.py:329\n\n\n\n                    INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/24/24 14:17:58] WARNING  This pipeline output 'conformal_prediction' will cost you an I/O       compiler.py:131\n                             operation without being used by current app, please consider freeing                  \n                             it. the pipeline outputs that are needed by the current pipeline                      \n                             namespace (None) are : ['predictions', 'integrated_gradients']                        \n\n\n\n                    WARNING  This pipeline output 'prediction' will cost you an I/O operation       compiler.py:131\n                             without being used by current app, please consider freeing it. the                    \n                             pipeline outputs that are needed by the current pipeline namespace                    \n                             (None) are : ['predictions', 'integrated_gradients']                                  \n\n\n\n                    WARNING  This pipeline output 'integrated_gradients' will cost you an I/O       compiler.py:117\n                             operation, please consider freeing it (making it a MemoryDataset).                    \n\n\n\n                    WARNING  This pipeline output 'predictions' will cost you an I/O operation,     compiler.py:117\n                             please consider freeing it (making it a MemoryDataset).                               \n\n\n\n                    WARNING  This pipeline output 'normalized_img' will cost you an I/O operation   compiler.py:131\n                             without being used by current app, please consider freeing it. the                    \n                             pipeline outputs that are needed by the current pipeline namespace                    \n                             (None) are : ['predictions', 'integrated_gradients']                                  \n\n\n\n                    WARNING  This pipeline output 'resized_img' will cost you an I/O operation      compiler.py:131\n                             without being used by current app, please consider freeing it. the                    \n                             pipeline outputs that are needed by the current pipeline namespace                    \n                             (None) are : ['predictions', 'integrated_gradients']                                  \n\n\n\n                    INFO     catalog compilation completed for the namespace 'None'. Here is the     context.py:147\n                             report:                                                                               \n                               - Input datasets to be replaced/rendered at iteration time:                         \n                             {'inference_sample'}                                                                  \n                               - Output datasets that hold the results of a run at iteration time:                 \n                             {'predictions', 'integrated_gradients'}                                               \n                               - Parameter datasets to be replaced/rendered at iteration time: set()               \n                               - Artifact datasets to be materialized (preloader as memory dataset)                \n                             at compile time: {'params:show_figures', 'best_model', 'cp_predictor',                \n                             'params:img_size', 'params:normal_mean', 'params:ood_threshold',                      \n                             'params:device', 'params:normal_std', 'ood_detector'}                                 \n                               - Template datasets to be rendered at iteration time: set()                         \n                                                                                                                   \n\n\n\n                    INFO     Loading artifacts datasets as MemoryDataset ...                         context.py:166\n\n\n\n                    INFO     Loading params:show_figures as a MemoryDataset                          context.py:181\n\n\n\n                    INFO     Loading best_model as a MemoryDataset                                   context.py:181\n\n\n\n                    INFO     Loading cp_predictor as a MemoryDataset                                 context.py:181\n\n\n\n                    INFO     Loading params:img_size as a MemoryDataset                              context.py:181\n\n\n\n                    INFO     Loading params:normal_mean as a MemoryDataset                           context.py:181\n\n\n\n                    INFO     Loading params:ood_threshold as a MemoryDataset                         context.py:181\n\n\n\n                    INFO     Loading params:device as a MemoryDataset                                context.py:181\n\n\n\n                    INFO     Loading params:normal_std as a MemoryDataset                            context.py:181\n\n\n\n                    INFO     Loading ood_detector as a MemoryDataset                                 context.py:181\n\n\n\n                    INFO     Catalog compilation completed.                                          context.py:169\n\n\n\n                    INFO     BooterApp execution completed.                                           adapter.py:86\n\n\n\n\nimport PIL\n\n\nimage = PIL.Image.open(\"../data/01_raw/inference_sample.png\")\n\n\noutput = session.run(inputs={\"inference_sample\": image})\n\n[11/24/24 14:15:54] INFO     Running iteration 8d3111d3618e421d8d2cd9f13db33681                       session.py:97\n\n\n\n                    INFO     Injecting 'inference_sample' input into the catalog                    renderer.py:136\n\n\n\n                    INFO     Using synchronous mode for loading and saving data. Use the    sequential_runner.py:67\n                             --async flag for potential performance gains.                                         \n                             https://docs.kedro.org/en/stable/nodes_and_pipelines/run_a_pip                        \n                             eline.html#load-and-save-asynchronously                                               \n\n\n\n                    INFO     Loading data from inference_sample (MemoryDataset)...              data_catalog.py:389\n\n\n\n                    INFO     Running node: prepare_data_for_ood_node:                                   node.py:367\n                             prepare_data_for_ood([inference_sample]) -&gt; [img_for_ood]                             \n\n\n\n                    INFO     Saving data to img_for_ood (MemoryDataset)...                      data_catalog.py:431\n\n\n\n                    INFO     Completed 1 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Loading data from inference_sample (MemoryDataset)...              data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:img_size (MemoryDataset)...               data_catalog.py:389\n\n\n\n                    INFO     Running node: resize_image_node_inference:                                 node.py:367\n                             resize_image([inference_sample;params:img_size]) -&gt; [resized_img]                     \n\n\n\n                    INFO     Saving data to resized_img (PickleDataset)...                      data_catalog.py:431\n\n\n\n                    INFO     Completed 2 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Loading data from resized_img (PickleDataset)...                   data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_mean (MemoryDataset)...            data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_std (MemoryDataset)...             data_catalog.py:389\n\n\n\n                    INFO     Running node: normalize_image_node_inference:                              node.py:367\n                             normalize_image([resized_img;params:normal_mean;params:normal_std]) -&gt;                \n                             [normalized_img]                                                                      \n\n\n\n                    INFO     Saving data to normalized_img (PickleDataset)...                   data_catalog.py:431\n\n\n\n                    INFO     Completed 3 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Loading data from img_for_ood (MemoryDataset)...                   data_catalog.py:389\n\n\n\n                    INFO     Loading data from ood_detector (MemoryDataset)...                  data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:ood_threshold (MemoryDataset)...          data_catalog.py:389\n\n\n\n                    INFO     Running node: ood_detection_node:                                          node.py:367\n                             ood_detection([img_for_ood;ood_detector;params:ood_threshold]) -&gt; None                \n\n\n\n                    INFO     Completed 4 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Loading data from best_model (MemoryDataset)...                    data_catalog.py:389\n\n\n\n                    INFO     Loading data from normalized_img (PickleDataset)...                data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:device (MemoryDataset)...                 data_catalog.py:389\n\n\n\n                    INFO     Running node: predict: predict([best_model;normalized_img;params:device])  node.py:367\n                             -&gt; [prediction]                                                                       \n\n\n\n                    INFO     Saving data to prediction (PickleDataset)...                       data_catalog.py:431\n\n\n\n                    INFO     Completed 5 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Loading data from prediction (PickleDataset)...                    data_catalog.py:389\n\n\n\n                    INFO     Loading data from cp_predictor (MemoryDataset)...                  data_catalog.py:389\n\n\n\n                    INFO     Running node: conformal_prediction_node:                                   node.py:367\n                             conformal_prediction([prediction;cp_predictor]) -&gt; [conformal_prediction]             \n\n\n\n                    INFO     Saving data to conformal_prediction (PickleDataset)...             data_catalog.py:431\n\n\n\n                    INFO     Completed 6 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Loading data from best_model (MemoryDataset)...                    data_catalog.py:389\n\n\n\n                    INFO     Loading data from normalized_img (PickleDataset)...                data_catalog.py:389\n\n\n\n                    INFO     Loading data from resized_img (PickleDataset)...                   data_catalog.py:389\n\n\n\n                    INFO     Loading data from conformal_prediction (PickleDataset)...          data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:show_figures (MemoryDataset)...           data_catalog.py:389\n\n\n\n                    INFO     Running node: integrated_gradients_node:                                   node.py:367\n                             integrated_gradients([best_model;normalized_img;resized_img;conformal_pred            \n                             iction;params:show_figures]) -&gt; [integrated_gradients;predictions]                    \n\n\n\n[11/24/24 14:15:55] INFO     Saving data to integrated_gradients (PickleDataset)...             data_catalog.py:431\n\n\n\n                    INFO     Saving data to predictions (PickleDataset)...                      data_catalog.py:431\n\n\n\n                    INFO     Completed 7 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Loading data from conformal_prediction (PickleDataset)...          data_catalog.py:389\n\n\n\n                    INFO     Running node: log_prediction_node: log_prediction([conformal_prediction])  node.py:367\n                             -&gt; None                                                                               \n\n\n\n                    INFO     Prediction: [5]                                                            nodes.py:92\n\n\n\n                    INFO     Completed 8 out of 8 tasks                                     sequential_runner.py:93\n\n\n\n                    INFO     Pipeline execution completed successfully.                               runner.py:125\n\n\n\n                    INFO     Loading data from integrated_gradients (PickleDataset)...          data_catalog.py:389\n\n\n\n                    INFO     Loading data from predictions (PickleDataset)...                   data_catalog.py:389\n\n\n\n                    INFO     Iteration 8d3111d3618e421d8d2cd9f13db33681 completed                    session.py:116\n\n\n\n\noutput\n\n\n\n\n\n{'integrated_gradients': [&lt;Figure size 600x600 with 2 Axes&gt;], 'predictions': [5]}"
  },
  {
    "objectID": "notebooks/DermaMnist.html",
    "href": "notebooks/DermaMnist.html",
    "title": "Importing data from medmnist",
    "section": "",
    "text": "To configure the data catalog, run the below cells. These cells will download the data from the medmnist dataset and save it in the data directory as a dataframe encapsuled in a pickle file.\n\n# !pip install medmnist\n\n\nimport numpy as np\nfrom medmnist import DermaMNIST\n\n\ndata_train = DermaMNIST(split=\"train\", download=True, root=\"../data/01_raw/\")\n\nUsing downloaded and verified file: ../data/01_raw/dermamnist.npz\n\n\n\nds = np.load(\"../data/01_raw/dermamnist.npz\")\n\n\ntrain_images = ds[\"train_images\"]\ntrain_labels = ds[\"train_labels\"]\nval_images = ds[\"val_images\"]\nval_labels = ds[\"val_labels\"]\ntest_images = ds[\"test_images\"]\ntest_labels = ds[\"test_labels\"]\n\n\n# def get_class_imbalance():\n#     global train_labels, val_labels, test_labels\n\n#     # Combine all labels to assess overall imbalance\n#     train_labels_list = train_labels.ravel().tolist()\n#     val_labels_list = val_labels.ravel().tolist()\n#     test_labels_list = test_labels.ravel().tolist()\n#     all_labels = np.concatenate([train_labels_list, val_labels_list, test_labels_list])\n\n#     # Function to calculate class distribution\n#     def calculate_class_distribution(labels):\n#         print(type(labels))\n#         class_counts = Counter(labels)  # Count occurrences of each class\n#         total_samples = len(labels)\n#         class_distribution = {\n#             cls: count / total_samples for cls, count in class_counts.items()\n#         }  # Normalize\n#         return class_counts, class_distribution\n\n#     # Function to calculate imbalance ratio\n#     def calculate_imbalance_ratio(class_counts):\n#         min_class_count = min(class_counts.values())\n#         max_class_count = max(class_counts.values())\n#         return max_class_count / min_class_count\n\n#     # Function to calculate entropy of class distribution\n#     def calculate_class_entropy(class_distribution):\n#         return -sum(p * np.log2(p) for p in class_distribution.values() if p &gt; 0)\n\n#     # Calculate metrics for train, val, and test sets\n#     train_counts, train_distribution = calculate_class_distribution(train_labels_list)\n#     val_counts, val_distribution = calculate_class_distribution(val_labels_list)\n#     test_counts, test_distribution = calculate_class_distribution(test_labels_list)\n#     overall_counts, overall_distribution = calculate_class_distribution(all_labels)\n\n#     # Imbalance ratios\n#     train_imbalance = calculate_imbalance_ratio(train_counts)\n#     val_imbalance = calculate_imbalance_ratio(val_counts)\n#     test_imbalance = calculate_imbalance_ratio(test_counts)\n\n#     # Class entropies\n#     train_entropy = calculate_class_entropy(train_distribution)\n#     val_entropy = calculate_class_entropy(val_distribution)\n#     test_entropy = calculate_class_entropy(test_distribution)\n\n#     # Display results\n#     print(\"Train Set:\")\n#     print(f\"Class Counts: {train_counts}\")\n#     print(f\"Imbalance Ratio: {train_imbalance:.2f}\")\n#     print(f\"Entropy: {train_entropy:.2f}\\n\")\n\n#     print(\"Validation Set:\")\n#     print(f\"Class Counts: {val_counts}\")\n#     print(f\"Imbalance Ratio: {val_imbalance:.2f}\")\n#     print(f\"Entropy: {val_entropy:.2f}\\n\")\n\n#     print(\"Test Set:\")\n#     print(f\"Class Counts: {test_counts}\")\n#     print(f\"Imbalance Ratio: {test_imbalance:.2f}\")\n#     print(f\"Entropy: {test_entropy:.2f}\\n\")\n\n#     # Plot class distribution\n#     def plot_class_distribution(class_counts, title):\n#         classes = list(class_counts.keys())\n#         counts = list(class_counts.values())\n#         plt.bar(classes, counts, color=\"skyblue\")\n#         plt.xlabel(\"Class\")\n#         plt.ylabel(\"Count\")\n#         plt.title(title)\n#         plt.show()\n\n#     plot_class_distribution(train_counts, \"Train Set Class Distribution\")\n#     plot_class_distribution(val_counts, \"Validation Set Class Distribution\")\n#     plot_class_distribution(test_counts, \"Test Set Class Distribution\")\n\n\n# import numpy as np\n# get_class_imbalance()\n\n\nfrom imblearn.over_sampling import SMOTE\n\n# print(train_images.shape)\nX = train_images.reshape(train_images.shape[0], -1)  # Flatten images\ny = train_labels\n\n(7007, 28, 28, 3)\n\n\n\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n\nX_resampled.shape\n\n\n\n\n\n(32851, 2352)\n\n\n\n\nX_resampled.reshape(-1, 28, 28, 3).shape\n\n\n\n\n\n(32851, 28, 28, 3)\n\n\n\n\ny_resampled.shape\n\n\n\n\n\n(32851,)\n\n\n\n\n# print(X_resampled[0])\n\n[158 111 117 ... 173 145 142]\n\n\n\ntrain_images = X_resampled.reshape(\n    -1, 28, 28, 3\n)  # Replace 28, 28 with actual dimensions\ntrain_labels = y_resampled\n# print(len(train_images))\n\n32851\n\n\n\n# get_class_imbalance()\n\n\ntrain_len = len(train_images)\nval_len = len(val_images)\ntest_len = len(test_images)\n# print(train_len)\n\n32851\n\n\n\n# generate ids array\ntrain_ids = [f\"train_{i}\" for i in range(train_len)]\nval_ids = [f\"val_{i}\" for i in range(val_len)]\ntest_ids = [f\"test_{i}\" for i in range(test_len)]\n\n\ntrain_images = list(train_images)\nval_images = list(val_images)\ntest_images = list(test_images)\n\n\ntrain_labels = list(train_labels)\nval_labels = list(val_labels)\ntest_labels = list(test_labels)\n\n\n# print(len(train_images))\n# print(len(train_labels))\n# print(len(train_ids))\n\n32851\n32851\n32851\n\n\n\n# construct a df for each of the splits\nimport pandas as pd\n\ntrain_df = pd.DataFrame(\n    {\n        \"id\": train_ids,\n        \"image\": train_images,\n        \"label\": train_labels,\n    }\n)\ntest_df = pd.DataFrame(\n    {\n        \"id\": test_ids,\n        \"image\": test_images,\n        \"label\": test_labels,\n    }\n)\nval_df = pd.DataFrame(\n    {\n        \"id\": val_ids,\n        \"image\": val_images,\n        \"label\": val_labels,\n    }\n)\n\n\n# train_df.to_pickle(\"../data/01_raw/train.pkl\")\n# test_df.to_pickle(\"../data/01_raw/test.pkl\")\n# val_df.to_pickle(\"../data/01_raw/val.pkl\")\n\n\ntrain_df_loaded = pd.read_pickle(\"../data/01_raw/train.pkl\")\n\n\ntrain_df_loaded[\"image\"].iloc[0].shape\n\n(28, 28, 3)\n\n\n\ndef normalizing_images(data: pd.DataFrame) -&gt; pd.DataFrame:\n    data[\"image\"] = data[\"image\"].apply(lambda x: x / 255.0)\n    return data\n\n\ntrain_df_loaded_new = normalizing_images(train_df_loaded)\ntrain_df_loaded_new\n\n\n\n\n\n\n\n\nid\nimage\nlabel\n\n\n\n\n0\ntrain_0\n[[[0.6196078431372549, 0.43529411764705883, 0....\n[0]\n\n\n1\ntrain_1\n[[[0.9019607843137255, 0.43529411764705883, 0....\n[5]\n\n\n2\ntrain_2\n[[[0.8980392156862745, 0.611764705882353, 0.67...\n[5]\n\n\n3\ntrain_3\n[[[0.8941176470588236, 0.4980392156862745, 0.4...\n[5]\n\n\n4\ntrain_4\n[[[0.8470588235294118, 0.7372549019607844, 0.7...\n[4]\n\n\n...\n...\n...\n...\n\n\n7002\ntrain_7002\n[[[0.788235294117647, 0.4980392156862745, 0.56...\n[5]\n\n\n7003\ntrain_7003\n[[[0.8941176470588236, 0.47843137254901963, 0....\n[5]\n\n\n7004\ntrain_7004\n[[[0.0784313725490196, 0.07058823529411765, 0....\n[2]\n\n\n7005\ntrain_7005\n[[[0.6705882352941176, 0.5529411764705883, 0.6...\n[5]\n\n\n7006\ntrain_7006\n[[[0.9372549019607843, 0.611764705882353, 0.69...\n[5]\n\n\n\n\n7007 rows × 3 columns\n\n\n\n\nfrom torchvision import transforms\n\n\ndef tensoring_resizing(data: pd.DataFrame) -&gt; pd.DataFrame:\n    transform = transforms.Compose(\n        [transforms.ToPILImage(), transforms.Resize((28, 28)), transforms.ToTensor()]\n    )\n\n    data[\"image\"] = data[\"image\"].apply(lambda x: transform(x).permute(1, 2, 0).numpy())\n    return data\n\n\ntrain_df_loaded_new = tensoring_resizing(train_df_loaded_new)\ntrain_df_loaded_new[\"image\"].iloc[0].shape\n\n(28, 28, 3)\n\n\n\n# %reload_ext kedro.ipython\n\n[11/20/24 11:51:51] INFO     Registered line magic '%reload_kedro'                                   __init__.py:58\n\n\n\n                    INFO     Registered line magic '%load_node'                                      __init__.py:60\n\n\n\n                    INFO     Resolved project path as: c:\\Users\\Admin\\Desktop\\onco-derm-ai.         __init__.py:175\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[11/20/24 11:51:52] INFO     Registering new custom resolver: 'km.random_name'                    mlflow_hook.py:62\n\n\n\n                    WARNING  No 'mlflow.yml' config file found in environment. Default            mlflow_hook.py:75\n                             configuration will be used. Use ``kedro mlflow init`` command in CLI                  \n                             to customize the configuration.                                                       \n\n\n\n                    INFO     The 'tracking_uri' key in mlflow.yml is relative            kedro_mlflow_config.py:260\n                             ('server.mlflow_(tracking|registry)_uri = mlruns'). It is                             \n                             converted to a valid uri:                                                             \n                             'file:///C:/Users/Admin/Desktop/onco-derm-ai/mlruns'                                  \n\n\n\n                    WARNING  Malformed experiment '312469223918544298'. Detailed error Yaml file  file_store.py:331\n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n                             ╭─────────────── Traceback (most recent call last) ────────────────╮                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:327 in search_experiments             │                  \n                             │                                                                  │                  \n                             │    324 │   │   for exp_id in experiment_ids:                     │                  \n                             │    325 │   │   │   try:                                          │                  \n                             │    326 │   │   │   │   # trap and warn known issues, will raise  │                  \n                             │ ❱  327 │   │   │   │   exp = self._get_experiment(exp_id, view_t │                  \n                             │    328 │   │   │   │   if exp is not None:                       │                  \n                             │    329 │   │   │   │   │   experiments.append(exp)               │                  \n                             │    330 │   │   │   except MissingConfigException as e:           │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:421 in _get_experiment                │                  \n                             │                                                                  │                  \n                             │    418 │   │   │   │   f\"Could not find experiment with ID {expe │                  \n                             │    419 │   │   │   │   databricks_pb2.RESOURCE_DOES_NOT_EXIST,   │                  \n                             │    420 │   │   │   )                                             │                  \n                             │ ❱  421 │   │   meta = FileStore._read_yaml(experiment_dir, FileS │                  \n                             │    422 │   │   meta[\"tags\"] = self.get_all_experiment_tags(exper │                  \n                             │    423 │   │   experiment = _read_persisted_experiment_dict(meta │                  \n                             │    424 │   │   if experiment_id != experiment.experiment_id:     │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1367 in _read_yaml                    │                  \n                             │                                                                  │                  \n                             │   1364 │   │   │   │   time.sleep(0.1 * (3 - attempts_remaining) │                  \n                             │   1365 │   │   │   │   return _read_helper(root, file_name, atte │                  \n                             │   1366 │   │                                                     │                  \n                             │ ❱ 1367 │   │   return _read_helper(root, file_name, attempts_rem │                  \n                             │   1368 │                                                         │                  \n                             │   1369 │   def _get_traces_artifact_dir(self, experiment_id, req │                  \n                             │   1370 │   │   return append_to_uri_path(                        │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1360 in _read_helper                  │                  \n                             │                                                                  │                  \n                             │   1357 │   │   \"\"\"                                               │                  \n                             │   1358 │   │                                                     │                  \n                             │   1359 │   │   def _read_helper(root, file_name, attempts_remain │                  \n                             │ ❱ 1360 │   │   │   result = read_yaml(root, file_name)           │                  \n                             │   1361 │   │   │   if result is not None or attempts_remaining = │                  \n                             │   1362 │   │   │   │   return result                             │                  \n                             │   1363 │   │   │   else:                                         │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\ut │                  \n                             │ ils\\file_utils.py:309 in read_yaml                               │                  \n                             │                                                                  │                  \n                             │    306 │                                                         │                  \n                             │    307 │   file_path = os.path.join(root, file_name)             │                  \n                             │    308 │   if not exists(file_path):                             │                  \n                             │ ❱  309 │   │   raise MissingConfigException(f\"Yaml file '{file_p │                  \n                             │    310 │   with codecs.open(file_path, mode=\"r\", encoding=ENCODI │                  \n                             │    311 │   │   return yaml.load(yaml_file, Loader=YamlSafeLoader │                  \n                             │    312                                                           │                  \n                             ╰──────────────────────────────────────────────────────────────────╯                  \n                             MissingConfigException: Yaml file                                                     \n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n\n\n\n                    WARNING  Malformed experiment '312469223918544298'. Detailed error Yaml file  file_store.py:331\n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n                             ╭─────────────── Traceback (most recent call last) ────────────────╮                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:327 in search_experiments             │                  \n                             │                                                                  │                  \n                             │    324 │   │   for exp_id in experiment_ids:                     │                  \n                             │    325 │   │   │   try:                                          │                  \n                             │    326 │   │   │   │   # trap and warn known issues, will raise  │                  \n                             │ ❱  327 │   │   │   │   exp = self._get_experiment(exp_id, view_t │                  \n                             │    328 │   │   │   │   if exp is not None:                       │                  \n                             │    329 │   │   │   │   │   experiments.append(exp)               │                  \n                             │    330 │   │   │   except MissingConfigException as e:           │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:421 in _get_experiment                │                  \n                             │                                                                  │                  \n                             │    418 │   │   │   │   f\"Could not find experiment with ID {expe │                  \n                             │    419 │   │   │   │   databricks_pb2.RESOURCE_DOES_NOT_EXIST,   │                  \n                             │    420 │   │   │   )                                             │                  \n                             │ ❱  421 │   │   meta = FileStore._read_yaml(experiment_dir, FileS │                  \n                             │    422 │   │   meta[\"tags\"] = self.get_all_experiment_tags(exper │                  \n                             │    423 │   │   experiment = _read_persisted_experiment_dict(meta │                  \n                             │    424 │   │   if experiment_id != experiment.experiment_id:     │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1367 in _read_yaml                    │                  \n                             │                                                                  │                  \n                             │   1364 │   │   │   │   time.sleep(0.1 * (3 - attempts_remaining) │                  \n                             │   1365 │   │   │   │   return _read_helper(root, file_name, atte │                  \n                             │   1366 │   │                                                     │                  \n                             │ ❱ 1367 │   │   return _read_helper(root, file_name, attempts_rem │                  \n                             │   1368 │                                                         │                  \n                             │   1369 │   def _get_traces_artifact_dir(self, experiment_id, req │                  \n                             │   1370 │   │   return append_to_uri_path(                        │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1360 in _read_helper                  │                  \n                             │                                                                  │                  \n                             │   1357 │   │   \"\"\"                                               │                  \n                             │   1358 │   │                                                     │                  \n                             │   1359 │   │   def _read_helper(root, file_name, attempts_remain │                  \n                             │ ❱ 1360 │   │   │   result = read_yaml(root, file_name)           │                  \n                             │   1361 │   │   │   if result is not None or attempts_remaining = │                  \n                             │   1362 │   │   │   │   return result                             │                  \n                             │   1363 │   │   │   else:                                         │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\ut │                  \n                             │ ils\\file_utils.py:309 in read_yaml                               │                  \n                             │                                                                  │                  \n                             │    306 │                                                         │                  \n                             │    307 │   file_path = os.path.join(root, file_name)             │                  \n                             │    308 │   if not exists(file_path):                             │                  \n                             │ ❱  309 │   │   raise MissingConfigException(f\"Yaml file '{file_p │                  \n                             │    310 │   with codecs.open(file_path, mode=\"r\", encoding=ENCODI │                  \n                             │    311 │   │   return yaml.load(yaml_file, Loader=YamlSafeLoader │                  \n                             │    312                                                           │                  \n                             ╰──────────────────────────────────────────────────────────────────╯                  \n                             MissingConfigException: Yaml file                                                     \n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n\n\n\n[11/20/24 11:51:53] WARNING  Malformed experiment '312469223918544298'. Detailed error Yaml file  file_store.py:331\n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n                             ╭─────────────── Traceback (most recent call last) ────────────────╮                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:327 in search_experiments             │                  \n                             │                                                                  │                  \n                             │    324 │   │   for exp_id in experiment_ids:                     │                  \n                             │    325 │   │   │   try:                                          │                  \n                             │    326 │   │   │   │   # trap and warn known issues, will raise  │                  \n                             │ ❱  327 │   │   │   │   exp = self._get_experiment(exp_id, view_t │                  \n                             │    328 │   │   │   │   if exp is not None:                       │                  \n                             │    329 │   │   │   │   │   experiments.append(exp)               │                  \n                             │    330 │   │   │   except MissingConfigException as e:           │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:421 in _get_experiment                │                  \n                             │                                                                  │                  \n                             │    418 │   │   │   │   f\"Could not find experiment with ID {expe │                  \n                             │    419 │   │   │   │   databricks_pb2.RESOURCE_DOES_NOT_EXIST,   │                  \n                             │    420 │   │   │   )                                             │                  \n                             │ ❱  421 │   │   meta = FileStore._read_yaml(experiment_dir, FileS │                  \n                             │    422 │   │   meta[\"tags\"] = self.get_all_experiment_tags(exper │                  \n                             │    423 │   │   experiment = _read_persisted_experiment_dict(meta │                  \n                             │    424 │   │   if experiment_id != experiment.experiment_id:     │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1367 in _read_yaml                    │                  \n                             │                                                                  │                  \n                             │   1364 │   │   │   │   time.sleep(0.1 * (3 - attempts_remaining) │                  \n                             │   1365 │   │   │   │   return _read_helper(root, file_name, atte │                  \n                             │   1366 │   │                                                     │                  \n                             │ ❱ 1367 │   │   return _read_helper(root, file_name, attempts_rem │                  \n                             │   1368 │                                                         │                  \n                             │   1369 │   def _get_traces_artifact_dir(self, experiment_id, req │                  \n                             │   1370 │   │   return append_to_uri_path(                        │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1360 in _read_helper                  │                  \n                             │                                                                  │                  \n                             │   1357 │   │   \"\"\"                                               │                  \n                             │   1358 │   │                                                     │                  \n                             │   1359 │   │   def _read_helper(root, file_name, attempts_remain │                  \n                             │ ❱ 1360 │   │   │   result = read_yaml(root, file_name)           │                  \n                             │   1361 │   │   │   if result is not None or attempts_remaining = │                  \n                             │   1362 │   │   │   │   return result                             │                  \n                             │   1363 │   │   │   else:                                         │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\ut │                  \n                             │ ils\\file_utils.py:309 in read_yaml                               │                  \n                             │                                                                  │                  \n                             │    306 │                                                         │                  \n                             │    307 │   file_path = os.path.join(root, file_name)             │                  \n                             │    308 │   if not exists(file_path):                             │                  \n                             │ ❱  309 │   │   raise MissingConfigException(f\"Yaml file '{file_p │                  \n                             │    310 │   with codecs.open(file_path, mode=\"r\", encoding=ENCODI │                  \n                             │    311 │   │   return yaml.load(yaml_file, Loader=YamlSafeLoader │                  \n                             │    312                                                           │                  \n                             ╰──────────────────────────────────────────────────────────────────╯                  \n                             MissingConfigException: Yaml file                                                     \n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n\n\n\n                    INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/20/24 11:51:54] INFO     Kedro project onco-derm-ai                                             __init__.py:141\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:142\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:148\n\n\n\n\nfrom kedro.io.data_catalog import DataCatalog\n\ncatalog = DataCatalog.from_config(\"../conf/base/catalog.yml\")\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[2], line 3\n      1 from kedro.io.data_catalog import DataCatalog\n----&gt; 3 catalog = DataCatalog.from_config(\"../conf/base/catalog.yml\")\n\nFile c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\data_catalog.py:314, in DataCatalog.from_config(cls, catalog, credentials, load_versions, save_version)\n    311 load_versions = copy.deepcopy(load_versions) or {}\n    312 user_default = {}\n--&gt; 314 for ds_name, ds_config in catalog.items():\n    315     if not isinstance(ds_config, dict):\n    316         raise DatasetError(\n    317             f\"Catalog entry '{ds_name}' is not a valid dataset configuration. \"\n    318             \"\\nHint: If this catalog entry is intended for variable interpolation, \"\n    319             \"make sure that the key is preceded by an underscore.\"\n    320         )\n\nAttributeError: 'str' object has no attribute 'items'\n\n\n\n\ncatalog.save(\"train_raw\", train_df)\n# catalog.save(\"test_raw\", test_df)\n# catalog.save(\"val_raw\", val_df)\n\n[11/18/24 23:22:45] INFO     Saving data to train_raw (PickleDataset)...                        data_catalog.py:581\n\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 catalog.save(\"train_raw\", train_df)                                                          │\n│   2 # catalog.save(\"test_raw\", test_df)                                                          │\n│   3 # catalog.save(\"val_raw\", val_df)                                                            │\n│   4                                                                                              │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\data_catalog.py:588 in save      │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\core.py:727 in save              │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\core.py:249 in save              │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro_datasets\\pickle\\pickle_dataset.py:2 │\n│ 25 in _save                                                                                      │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\core.py:704 in _get_save_path    │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nDatasetError: Save path \n'C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/train.pkl/2024-11-18T17.50.33.688Z/train.pkl' for \nPickleDataset(backend=pickle, filepath=C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/train.pkl, load_args={}, \nprotocol=file, save_args={}, version=Version(load=None, save='2024-11-18T17.50.33.688Z')) must not exist if \nversioning is enabled.\n\n\n\n\ndf = catalog.load(\"train_raw\")\n\n[11/20/24 11:52:04] INFO     Loading data from train_raw (PickleDataset)...                     data_catalog.py:539\n\n\n\n\n# df\n\n\n\n\n\n\n\n\n\n\n\nid\nimage\nlabel\n\n\n\n\n0\ntrain_0\n[[[158, 111, 117], [161, 116, 121], [164, 121,...\n[0]\n\n\n1\ntrain_1\n[[[230, 111, 105], [226, 107, 99], [225, 106, ...\n[5]\n\n\n2\ntrain_2\n[[[229, 156, 173], [229, 156, 173], [227, 156,...\n[5]\n\n\n3\ntrain_3\n[[[228, 127, 115], [224, 126, 113], [223, 125,...\n[5]\n\n\n4\ntrain_4\n[[[216, 188, 187], [217, 189, 188], [220, 192,...\n[4]\n\n\n...\n...\n...\n...\n\n\n7002\ntrain_7002\n[[[201, 127, 144], [199, 125, 142], [201, 124,...\n[5]\n\n\n7003\ntrain_7003\n[[[228, 122, 136], [229, 123, 137], [231, 126,...\n[5]\n\n\n7004\ntrain_7004\n[[[20, 18, 32], [54, 52, 66], [95, 94, 108], [...\n[2]\n\n\n7005\ntrain_7005\n[[[171, 141, 177], [172, 142, 178], [175, 143,...\n[5]\n\n\n7006\ntrain_7006\n[[[239, 156, 176], [235, 152, 172], [231, 149,...\n[5]\n\n\n\n\n7007 rows × 3 columns\n\n\n\n\ndf[\"image\"][0].shape\n\n\n\n\n\n(28, 28, 3)\n\n\n\n\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndevice\n\ndevice(type='cuda')\n\n\n\nimport pandas as pd\nfrom PIL import Image\n\ndata_augmentation = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(p=0.75),\n        transforms.RandomVerticalFlip(p=0.75),\n        transforms.RandomRotation(degrees=30),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.ToTensor(),\n    ]\n)\n\n\ndef augment_and_add_rows(original_df, augment_fn, num_augmented_per_image):\n    \"\"\"\n    Augments the images in the DataFrame and appends the augmented rows.\n\n    Args:\n        original_df (pd.DataFrame): Original DataFrame containing images and metadata.\n        augment_fn (callable): Augmentation function to apply.\n        num_augmented_per_image (int): Number of augmented images to create for each original image.\n\n    Returns:\n        pd.DataFrame: DataFrame with new augmented rows added.\n    \"\"\"\n    augmented_rows = []\n\n    for idx, row in original_df.iterrows():\n        original_image = row[\"image\"]\n        label = row[\"label\"]\n        image_id = row[\"id\"]\n\n        # Convert image array to PIL Image\n        pil_image = Image.fromarray(np.array(original_image, dtype=np.uint8))\n\n        for i in range(num_augmented_per_image):\n            # Apply augmentations\n            augmented_image = augment_fn(pil_image)\n            # Convert back to numpy array for storage\n            augmented_array = np.array(augmented_image.permute(1, 2, 0) * 255).astype(\n                np.uint8\n            )\n\n            # Create a new unique ID for the augmented image\n            new_id = f\"{image_id}_aug_{i}\"\n\n            # Add the new row\n            augmented_rows.append(\n                {\"id\": new_id, \"image\": augmented_array, \"label\": label}\n            )\n\n    # Convert augmented rows to a DataFrame\n    augmented_df = pd.DataFrame(augmented_rows)\n\n    # Append the augmented DataFrame to the original\n    return pd.concat([original_df, augmented_df], ignore_index=True)\n\n\naugmented_df = augment_and_add_rows(df, data_augmentation, num_augmented_per_image=2)\n\n# Check the resulting DataFrame\n# print(augmented_df.head())\n\n# augmented_df\n\n\n# print(f\"Original rows: {len(df)}, Augmented rows: {len(augmented_df) - len(df)}, Total rows: {len(augmented_df)}\")\n\n\n\n\n\n\n\n\n\n\n\nid\nimage\nlabel\n\n\n\n\n0\ntrain_0\n[[[158, 111, 117], [161, 116, 121], [164, 121,...\n[0]\n\n\n1\ntrain_1\n[[[230, 111, 105], [226, 107, 99], [225, 106, ...\n[5]\n\n\n2\ntrain_2\n[[[229, 156, 173], [229, 156, 173], [227, 156,...\n[5]\n\n\n3\ntrain_3\n[[[228, 127, 115], [224, 126, 113], [223, 125,...\n[5]\n\n\n4\ntrain_4\n[[[216, 188, 187], [217, 189, 188], [220, 192,...\n[4]\n\n\n...\n...\n...\n...\n\n\n21016\ntrain_7004_aug_1\n[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...\n[2]\n\n\n21017\ntrain_7005_aug_0\n[[[17, 17, 17], [17, 17, 17], [160, 127, 144],...\n[5]\n\n\n21018\ntrain_7005_aug_1\n[[[1, 1, 1], [131, 108, 140], [131, 108, 140],...\n[5]\n\n\n21019\ntrain_7006_aug_0\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n[5]\n\n\n21020\ntrain_7006_aug_1\n[[[5, 5, 5], [5, 5, 5], [5, 5, 5], [251, 178, ...\n[5]\n\n\n\n\n21021 rows × 3 columns\n\n\n\n\n# Access the shape of image present in last row\nlast_image = augmented_df[\"image\"].iloc[0]\n# len(last_image[0][0])\n# last_image\n# print(type(last_image))\n# print(type(augmented_df[\"image\"].iloc[-1]))\n\n&lt;class 'numpy.ndarray'&gt;\n&lt;class 'numpy.ndarray'&gt;\n\n\n\naugmented_df[\"image\"].iloc[-1].shape\n\n\n\n\n\n(28, 28, 3)"
  }
]