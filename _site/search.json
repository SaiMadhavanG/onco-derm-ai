[
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "Initialization",
    "section": "",
    "text": "Initialization\n\ncome up with a name\nkedro init\npre commit hooks\n\nlinting\ntesting\nquartodoc\n\ntesting - pytest, git pre commit hook\nauto documentation - quartodoc\n\n\n\nDocumentation\n\nproject card - sai\ndata card\nmodel card\nmlops card\n\n\n\nData Prep\n\nLoad data - kedro\nData versioning - kedro\nImage preprocessing - normalizing, tensorizing and resizing - torchvision - srini\nData augmentation - torchvision\n\nData quality - ?, greater expectation\nright to erasure, forgetting - kedro pipeline\n\n\n\n\nTraining\n\nModel training - Resnet - PyTorch\nModel eval - Sk classification report\nModel versioning - mlflow\nhyperparameter tuning - optuna/sklearn search\nautomatic reports - quarto and plotting libs\nModel pruning - pytorch\nadversarial robustness - auto_lirpa\nright to erasure, model retraining - kedro pipeline\n\n\n\nInference\n\nexplainability - deel\nconfidence calibration - deel, ?\nOOD detection - ?\nConformal predictions - deel\n\n\n\nDeployment\n\nmodel containerization - mlflow, docker\nauto model deployment - github actions/cloud provider\ndeployment side eval\ndata drift\nauto retraining triggers\n\n\n\nLLM\n\nllm set up\nllm prompt config\nllm deployment\n\n\n\nFront End\n\nFrontend dashboard - react, bootstrap\nChatbot window"
  },
  {
    "objectID": "notebooks/RestNet.html",
    "href": "notebooks/RestNet.html",
    "title": "Fine tune Resnet-18",
    "section": "",
    "text": "This code is for fine-tuning the ResNet model on the DermaMnist dataset.\nThe DermaMnist dataset is a collection of dermatological images used for skin disease classification. Fine-tuning involves taking a pre-trained ResNet model and adapting it to the specific task of classifying images in the DermaMnist dataset.\nThe process typically includes: - Loading the pre-trained ResNet model. - Modifying the final layers to match the number of classes in the DermaMnist dataset. - Training the modified model on the DermaMnist dataset. - Evaluating the performance of the fine-tuned model.\nThis approach leverages the pre-trained features of the ResNet model, which can lead to better performance and faster convergence compared to training a model from scratch.\n\n# import torch\n# import torchvision.models as models\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\n\n# Load the pre-trained ResNet-18 model\nresnet18 = models.resnet18(pretrained=True)\n\n# Print the model architecture\n# print(resnet18.fc)\n\n\nclass DermaMNISTDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        image = self.dataframe[\"image\"][idx]  # (28, 28, 3) numpy array\n        # image = (image * 255).astype(np.uint8)  # Convert to uint8 for transforms\n        label = self.dataframe[\"label\"][idx]\n\n        # Convert numpy image to PIL Image for applying transforms\n        image = transforms.ToPILImage()(image)\n        image = transforms.ToTensor()(image)\n        if self.transform:\n            image = self.transform(image)\n\n        # print(type(label))\n        # print(type(image))\n        return image, label\n\n\n# %reload_ext kedro.ipython\n\n[11/08/24 22:03:27] INFO     Registered line magic '%reload_kedro'                                   __init__.py:58\n\n\n\n                    INFO     Registered line magic '%load_node'                                      __init__.py:60\n\n\n\n                    INFO     Resolved project path as: c:\\Users\\Admin\\Desktop\\onco-derm-ai.         __init__.py:175\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n                    INFO     Registering new custom resolver: 'km.random_name'                    mlflow_hook.py:62\n\n\n\n                    WARNING  No 'mlflow.yml' config file found in environment. Default            mlflow_hook.py:75\n                             configuration will be used. Use ``kedro mlflow init`` command in CLI                  \n                             to customize the configuration.                                                       \n\n\n\n                    INFO     The 'tracking_uri' key in mlflow.yml is relative            kedro_mlflow_config.py:260\n                             ('server.mlflow_(tracking|registry)_uri = mlruns'). It is                             \n                             converted to a valid uri:                                                             \n                             'file:///C:/Users/Admin/Desktop/onco-derm-ai/mlruns'                                  \n\n\n\n[11/08/24 22:03:28] INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/08/24 22:03:29] INFO     Kedro project onco-derm-ai                                             __init__.py:141\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:142\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:148\n\n\n\n\nfrom kedro.io.data_catalog import DataCatalog\n\ncatalog = DataCatalog.from_config(\"../conf/base/catalog.yml\")\n\n\n# catalog\n\n\n\n\n\n{'train_dataset': 'kedro.io.memory_dataset.MemoryDataset()',\n 'train_intermediate': 'kedro.io.memory_dataset.MemoryDataset()',\n 'train_raw': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/train.pkl'), \"\n              \"backend='pickle', protocol='file', load_args={}, save_args={}, \"\n              \"version=Version(load=None, save='2024-11-08T16.21.06.976Z'))\",\n 'val_raw': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/val.pkl'), \"\n            \"backend='pickle', protocol='file', load_args={}, save_args={}, \"\n            \"version=Version(load=None, save='2024-11-08T16.21.06.976Z'))\",\n 'test_raw': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/test.pkl'), \"\n             \"backend='pickle', protocol='file', load_args={}, save_args={}, \"\n             \"version=Version(load=None, save='2024-11-08T16.21.06.976Z'))\",\n 'pre-processed_train_data': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/data/02_intermediate/pre-processed_train_data.pkl'), \"\n                             \"backend='pickle', protocol='file', load_args={}, \"\n                             'save_args={}, version=Version(load=None, '\n                             \"save='2024-11-08T16.21.06.976Z'))\",\n 'image_classification_model': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/models/image_classification_model.pkl'), \"\n                               \"backend='pickle', protocol='file', \"\n                               'load_args={}, save_args={}, '\n                               'version=Version(load=None, '\n                               \"save='2024-11-08T16.21.06.976Z'))\",\n 'model_finetuned': \"kedro_datasets.pickle.pickle_dataset.PickleDataset(filepath=PurePosixPath('C:/Users/Admin/Desktop/onco-derm-ai/models/model_fintuned.pkl'), \"\n                    \"backend='pickle', protocol='file', load_args={}, \"\n                    'save_args={}, version=Version(load=None, '\n                    \"save='2024-11-08T16.21.06.976Z'))\",\n 'parameters': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;dict&gt;')\",\n 'params:model_name': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;str&gt;')\",\n 'params:num_epochs': \"kedro.io.memory_dataset.MemoryDataset(data='&lt;int&gt;')\"}\n\n\n\n\ntrain_data = catalog.load(\"pre-processed_train_data\")\n\n[11/08/24 22:03:40] INFO     Loading data from pre-processed_train_data (PickleDataset)...      data_catalog.py:539\n\n\n\n\n# train_data[\"image\"][0]\n\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),  # Resize to 224x224\n        # transforms.ToTensor(),             # Convert to tensor\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        ),  # Normalize\n    ]\n)\n\n# Load your dataset\ntrain_dataset = DermaMNISTDataset(train_data, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Initialize ResNet-18 model\nmodel = models.resnet18(pretrained=True)\nnum_classes = 7\nmodel.fc = nn.Linear(\n    model.fc.in_features, num_classes\n)  # Adjust final layer for DermaMNIST classes\n\n# Set up loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nnum_epochs = 10  # Adjust as needed\n\n# train_loader\n\n[11/08/24 22:03:45] WARNING  c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\torchvision\\mod warnings.py:109\n                             els\\_utils.py:208: UserWarning: The parameter 'pretrained' is                         \n                             deprecated since 0.13 and may be removed in the future, please use                    \n                             'weights' instead.                                                                    \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\torchvision\\mod warnings.py:109\n                             els\\_utils.py:223: UserWarning: Arguments other than a weight enum or                 \n                             `None` for 'weights' are deprecated since 0.13 and may be removed in                  \n                             the future. The current behavior is equivalent to passing                             \n                             `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use                            \n                             `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.                \n                               warnings.warn(msg)                                                                  \n                                                                                                                   \n\n\n\n\nimages = \"\"\nlabels = \"\"\noutputs = \"\"\nlost = \"\"\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, lbs in train_loader:\n        # print(type(images))\n        images, labels = i.to(device), lbs.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        # print(labels)\n        labels_output = labels.squeeze().long()\n        loss = criterion(outputs, labels_output)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n    # Save the model checkpoint\ntorch.save(model.state_dict(), \"resnet18_dermamnist.pth\")\n# print(\"Pre-training complete.\")\n\nEpoch [1/10], Loss: 0.8958\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:19                                                                                   │\n│                                                                                                  │\n│   16 │   │   loss.backward()                                                                     │\n│   17 │   │   optimizer.step()                                                                    │\n│   18 │   │                                                                                       │\n│ ❱ 19 │   │   running_loss += loss.item()                                                         │\n│   20 │                                                                                           │\n│   21 │   print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")    │\n│   22 │   # Save the model checkpoint                                                             │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nKeyboardInterrupt"
  },
  {
    "objectID": "reference/preprocess_data_input.html",
    "href": "reference/preprocess_data_input.html",
    "title": "preprocess_data_input",
    "section": "",
    "text": "preprocess_data_input\npreprocess_data_input(train_data)\nPreprocesses the input training data for the DermaMNIST dataset.\nThis function applies transformations, including resizing images to 224x224 pixels and normalizing them with specified mean and standard deviation values.\nArgs: train_data (pd.DataFrame): The input training data in the form of a pandas DataFrame.\nReturns: DermaMNISTDataset: The preprocessed training dataset ready for model training."
  },
  {
    "objectID": "reference/DermaMNISTDataset.html",
    "href": "reference/DermaMNISTDataset.html",
    "title": "DermaMNISTDataset",
    "section": "",
    "text": "DermaMNISTDataset\nDermaMNISTDataset(self, dataframe, transform=None)\nA custom Dataset class for the DermaMNIST dataset.\nArgs: dataframe (pd.DataFrame): A pandas DataFrame containing the dataset. It should have two columns: ‘image’ and ‘label’. ‘image’ should contain (28, 28, 3) numpy arrays. transform (callable, optional): Optional transform to be applied on a sample.\nAttributes: dataframe (pd.DataFrame): The dataframe containing the dataset. transform (callable): The transform to be applied on a sample.\nMethods: len(): Returns the length of the dataset. getitem(idx): Returns the image and label at the given index."
  },
  {
    "objectID": "reference/model_select.html",
    "href": "reference/model_select.html",
    "title": "model_select",
    "section": "",
    "text": "model_select\nmodel_select(model_name, num_outputs=7, pretrained=False)\nSelects and returns a pre-trained model based on the provided model name.\nArgs: model_name (str): The name of the model to select. Supported values are “ResNet18” and “VGG16”. num_outputs (int): The number of output classes for the model. pretrained (bool): If True, loads a pre-trained model.\nReturns: models: The selected pre-trained model.\nRaises: ValueError: If the provided model name is not supported."
  },
  {
    "objectID": "reference/model_finetune.html",
    "href": "reference/model_finetune.html",
    "title": "model_finetune",
    "section": "",
    "text": "model_finetune\nmodel_finetune(\n    train_dataset,\n    val_dataset,\n    model_name,\n    train_params,\n    device='cpu',\n)\nFinetunes a pre-trained model on the given training dataset.\nArgs: train_dataset (DermaMNISTDataset): The dataset to train the model on. model_name (str): The name of the model to finetune. train_params (dict): A dictionary containing the training parameters. device (str): The device to train the model on (e.g., “cpu” or “cuda”).\nReturns: dict: The state dictionary of the finetuned model."
  },
  {
    "objectID": "reference/evaluate_model.html",
    "href": "reference/evaluate_model.html",
    "title": "evaluate_model",
    "section": "",
    "text": "evaluate_model\nevaluate_model(\n    model_name,\n    model_state_dict,\n    test_dataset,\n    batch_size,\n    device='cpu',\n)\nEvaluates a pre-trained model on the given test dataset.\nArgs: model_name (str): The name of the model being evaluated. model_state_dict (dict): The state dictionary of the pre-trained model. test_dataset (DermaMNISTDataset): The dataset to evaluate the model on. batch_size (int): The batch size to use during evaluation. device (str): The device to evaluate the model on (e.g., “cpu” or “cuda”).\nReturns: dict: The classification report of the model’s performance."
  },
  {
    "objectID": "rough.html",
    "href": "rough.html",
    "title": "OncoDerm AI",
    "section": "",
    "text": "%load_ext kedro.ipython\n\n[11/18/24 23:55:03] INFO     Using 'conf/logging.yml' as logging configuration. You can change this __init__.py:270\n                             by setting the KEDRO_LOGGING_CONFIG environment variable accordingly.                 \n\n\n\n                    INFO     Registered line magic '%reload_kedro'                                   __init__.py:61\n\n\n\n                    INFO     Registered line magic '%load_node'                                      __init__.py:63\n\n\n\n                    INFO     Resolved project path as: /home/saimadhavang/sem7/mlpe/onco-derm-ai.   __init__.py:178\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[11/18/24 23:55:07] WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro_viz/__init__.py:13: KedroVizPythonVersionWarning:                      \n                             Please be advised that Kedro Viz is not yet fully                                     \n                                     compatible with the Python version you are currently using.                   \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/framework/project/__init__.py:347: UserWarning: The                    \n                             'onco_derm_ai.pipelines.model_inference' module does not expose a                     \n                             'create_pipeline' function, so no pipelines defined therein will be                   \n                             returned by 'find_pipelines'.                                                         \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/framework/project/__init__.py:347: UserWarning: The                    \n                             'onco_derm_ai.pipelines.ood_detection' module does not expose a                       \n                             'create_pipeline' function, so no pipelines defined therein will be                   \n                             returned by 'find_pipelines'.                                                         \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/framework/project/__init__.py:347: UserWarning: The                    \n                             'onco_derm_ai.pipelines.inf_data_preprocessing' module does not expose                \n                             a 'create_pipeline' function, so no pipelines defined therein will be                 \n                             returned by 'find_pipelines'.                                                         \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/18/24 23:55:08] INFO     Kedro project onco-derm-ai                                             __init__.py:144\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:145\n                             'pipelines'                                                                           \n\n\n\n[11/18/24 23:55:09] INFO     Registered line magic 'run_viz'                                        __init__.py:151\n\n\n\n\ntrain_raw = catalog.load(\"train_raw\")\n\n[11/18/24 23:55:16] INFO     Loading data from train_raw (PickleDataset)...                     data_catalog.py:389\n\n\n\n\nimport numpy as np\n\n\nnp.stack(train_raw['image'])\n\n\n\n\n\narray([[[[158, 111, 117],\n         [161, 116, 121],\n         [164, 121, 130],\n         ...,\n         [189, 160, 164],\n         [187, 158, 160],\n         [186, 157, 159]],\n        [[160, 113, 119],\n         [165, 120, 125],\n         [170, 129, 137],\n         ...,\n         [191, 162, 166],\n         [189, 160, 162],\n         [188, 159, 161]],\n        [[164, 119, 124],\n         [170, 128, 132],\n         [177, 137, 145],\n         ...,\n         [191, 162, 166],\n         [188, 162, 165],\n         [187, 161, 164]],\n        ...,\n        [[172, 142, 150],\n         [176, 147, 152],\n         [182, 150, 155],\n         ...,\n         [195, 167, 164],\n         [181, 153, 150],\n         [171, 143, 140]],\n        [[176, 147, 152],\n         [176, 147, 152],\n         [180, 148, 153],\n         ...,\n         [188, 160, 157],\n         [178, 150, 147],\n         [171, 143, 140]],\n        [[175, 146, 151],\n         [173, 144, 149],\n         [174, 142, 147],\n         ...,\n         [180, 152, 149],\n         [175, 147, 144],\n         [173, 145, 142]]],\n       [[[230, 111, 105],\n         [226, 107,  99],\n         [225, 106,  98],\n         ...,\n         [217, 113, 102],\n         [215, 111, 100],\n         [213, 109,  98]],\n        [[225, 106, 100],\n         [221, 102,  94],\n         [220, 102,  92],\n         ...,\n         [218, 114, 103],\n         [216, 112, 101],\n         [215, 111, 100]],\n        [[222, 103,  97],\n         [218,  99,  91],\n         [217, 100,  90],\n         ...,\n         [219, 115, 104],\n         [217, 115, 103],\n         [216, 114, 102]],\n        ...,\n        [[212, 119, 102],\n         [214, 121, 104],\n         [215, 122, 104],\n         ...,\n         [213, 121, 100],\n         [212, 122,  98],\n         [211, 121,  97]],\n        [[212, 119, 101],\n         [214, 121, 103],\n         [217, 122, 104],\n         ...,\n         [213, 122, 101],\n         [212, 122,  98],\n         [211, 121,  97]],\n        [[212, 119, 101],\n         [214, 121, 103],\n         [217, 122, 104],\n         ...,\n         [213, 122, 101],\n         [212, 122,  98],\n         [211, 121,  97]]],\n       [[[229, 156, 173],\n         [229, 156, 173],\n         [227, 156, 174],\n         ...,\n         [236, 174, 177],\n         [239, 172, 179],\n         [238, 171, 178]],\n        [[231, 158, 175],\n         [230, 157, 174],\n         [228, 157, 175],\n         ...,\n         [233, 171, 174],\n         [235, 170, 176],\n         [235, 168, 175]],\n        [[231, 160, 176],\n         [230, 159, 175],\n         [229, 158, 174],\n         ...,\n         [236, 174, 179],\n         [237, 172, 178],\n         [237, 172, 178]],\n        ...,\n        [[195, 147, 159],\n         [197, 149, 161],\n         [200, 152, 164],\n         ...,\n         [218, 161, 170],\n         [217, 160, 169],\n         [217, 160, 169]],\n        [[196, 149, 159],\n         [197, 150, 160],\n         [198, 151, 161],\n         ...,\n         [216, 160, 169],\n         [216, 160, 169],\n         [216, 160, 169]],\n        [[197, 150, 158],\n         [197, 150, 158],\n         [197, 150, 158],\n         ...,\n         [216, 160, 169],\n         [216, 160, 169],\n         [215, 159, 168]]],\n       ...,\n       [[[ 20,  18,  32],\n         [ 54,  52,  66],\n         [ 95,  94, 108],\n         ...,\n         [ 96,  98, 110],\n         [ 48,  51,  60],\n         [ 16,  19,  28]],\n        [[ 49,  47,  61],\n         [ 78,  76,  90],\n         [114, 112, 126],\n         ...,\n         [118, 120, 132],\n         [ 86,  86,  98],\n         [ 58,  61,  70]],\n        [[ 89,  85,  99],\n         [110, 106, 120],\n         [136, 132, 147],\n         ...,\n         [141, 141, 153],\n         [125, 123, 136],\n         [106, 106, 118]],\n        ...,\n        [[122, 116, 130],\n         [137, 133, 147],\n         [151, 148, 159],\n         ...,\n         [149, 136, 145],\n         [133, 120, 129],\n         [114, 101, 110]],\n        [[ 73,  65,  78],\n         [108, 102, 114],\n         [144, 138, 150],\n         ...,\n         [137, 124, 133],\n         [114, 101, 110],\n         [ 90,  77,  86]],\n        [[ 29,  21,  34],\n         [ 77,  69,  82],\n         [127, 121, 133],\n         ...,\n         [123, 110, 119],\n         [ 94,  81,  90],\n         [ 65,  52,  61]]],\n       [[[171, 141, 177],\n         [172, 142, 178],\n         [175, 143, 180],\n         ...,\n         [169, 129, 156],\n         [169, 130, 157],\n         [169, 130, 157]],\n        [[175, 145, 181],\n         [176, 146, 180],\n         [178, 146, 183],\n         ...,\n         [175, 135, 162],\n         [174, 135, 162],\n         [174, 135, 162]],\n        [[182, 151, 185],\n         [181, 150, 182],\n         [182, 151, 185],\n         ...,\n         [180, 140, 167],\n         [179, 139, 166],\n         [177, 137, 164]],\n        ...,\n        [[171, 128, 158],\n         [171, 128, 158],\n         [170, 126, 159],\n         ...,\n         [173, 142, 176],\n         [171, 140, 174],\n         [169, 138, 172]],\n        [[177, 136, 166],\n         [173, 132, 162],\n         [168, 127, 159],\n         ...,\n         [169, 138, 172],\n         [168, 134, 169],\n         [166, 132, 167]],\n        [[179, 140, 169],\n         [174, 135, 164],\n         [168, 127, 159],\n         ...,\n         [169, 138, 172],\n         [169, 135, 170],\n         [167, 133, 168]]],\n       [[[239, 156, 176],\n         [235, 152, 172],\n         [231, 149, 171],\n         ...,\n         [241, 160, 179],\n         [237, 158, 177],\n         [236, 157, 176]],\n        [[236, 153, 173],\n         [234, 151, 171],\n         [232, 149, 169],\n         ...,\n         [241, 160, 179],\n         [238, 159, 178],\n         [237, 158, 177]],\n        [[235, 150, 169],\n         [235, 150, 169],\n         [235, 150, 169],\n         ...,\n         [239, 160, 179],\n         [238, 159, 178],\n         [237, 158, 177]],\n        ...,\n        [[231, 168, 179],\n         [232, 166, 178],\n         [230, 162, 173],\n         ...,\n         [226, 154, 168],\n         [227, 155, 169],\n         [228, 156, 170]],\n        [[232, 166, 178],\n         [232, 166, 178],\n         [231, 163, 174],\n         ...,\n         [227, 156, 172],\n         [225, 154, 170],\n         [224, 153, 169]],\n        [[232, 166, 178],\n         [231, 165, 177],\n         [233, 163, 174],\n         ...,\n         [228, 157, 173],\n         [225, 154, 170],\n         [222, 151, 167]]]], dtype=uint8)\n\n\n\n\ntrain_raw['label'].to_numpy(dtype=np.int32)\n\n\n\n\n\narray([0, 5, 5, ..., 2, 5, 5], dtype=int32)\n\n\n\n\ndetector = catalog.load(\"ood_detector\")\n\n[11/18/24 22:37:46] INFO     Loading data from ood_detector (PickleDataset)...                  data_catalog.py:389\n\n\n\n\ndetector\n\n\n\n\n\n&lt;pytorch_ood.detector.mmahalanobis.MultiMahalanobis object at 0x7fecf60c82c0&gt;\n\n\n\n\nsample_img = catalog.load(\"inference_sample\")\n\n[11/18/24 15:46:37] INFO     Loading data from inference_sample (ImageDataset)...               data_catalog.py:389\n\n\n\n\n%load_node resize_image_node_inference\n\n[11/18/24 15:48:23] WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/ipython/__init__.py:319: UserWarning: This is an                       \n                             experimental feature, only Jupyter Notebook (&gt;7.0), Jupyter Lab,                      \n                             IPython, and VSCode Notebook are supported. If you encounter                          \n                             unexpected behaviour or would like to suggest feature enhancements,                   \n                             add it under this github issue                                                        \n                             https://github.com/kedro-org/kedro/issues/3580                                        \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Loading node definition from                                           __init__.py:360\n                             /home/saimadhavang/sem7/mlpe/onco-derm-ai/src/onco_derm_ai/pipelines/i                \n                             nf_data_preprocessing/nodes.py                                                        \n\n\n\n\n# Prepare necessary inputs for debugging\n# All debugging inputs must be defined in your project catalog\nimg = catalog.load(\"inference_sample\")\nsize = catalog.load(\"params:img_size\")\n\nfrom typing import Tuple\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F\nfrom PIL import Image\n\ndef resize_image(img: Image, size: Tuple[int, int]) -&gt; torch.Tensor:\n    \"\"\"\n    Resize image.\n    Args:\n        img: Image to be resized.\n        size: New size for the image.\n    Returns:\n        Resized image.\n    \"\"\"\n    img = np.array(img.convert(\"RGB\")).astype(np.float32)\n    img = F.to_tensor(img)/255.0\n    img = F.resize(img, size)\n    return img\n\n\nresized_img = resize_image(img, size)\n\n[11/18/24 15:55:53] INFO     Loading data from inference_sample (ImageDataset)...               data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:img_size (MemoryDataset)...               data_catalog.py:389\n\n\n\n\n%load_node normalize_image_node_inference\n\n[11/18/24 15:48:34] WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/ipython/__init__.py:319: UserWarning: This is an                       \n                             experimental feature, only Jupyter Notebook (&gt;7.0), Jupyter Lab,                      \n                             IPython, and VSCode Notebook are supported. If you encounter                          \n                             unexpected behaviour or would like to suggest feature enhancements,                   \n                             add it under this github issue                                                        \n                             https://github.com/kedro-org/kedro/issues/3580                                        \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Loading node definition from                                           __init__.py:360\n                             /home/saimadhavang/sem7/mlpe/onco-derm-ai/src/onco_derm_ai/pipelines/i                \n                             nf_data_preprocessing/nodes.py                                                        \n\n\n\n\n# Prepare necessary inputs for debugging\n# All debugging inputs must be defined in your project catalog\nmean = catalog.load(\"params:normal_mean\")\nstd = catalog.load(\"params:normal_std\")\n\nfrom typing import Tuple\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F\nfrom PIL import Image\n\ndef normalize_image(\n    img: torch.Tensor, mean: Tuple[float, float, float], std: Tuple[float, float, float]\n) -&gt; torch.tensor:\n    \"\"\"\n    Normalize image.\n    Args:\n        img: Image to be normalized.\n        mean: Mean value for normalization.\n        std: Standard deviation for normalization.\n    Returns:\n        Normalized image.\n    \"\"\"\n    img = F.normalize(img, mean, std)\n    return img\n\n\noutput = normalize_image(resized_img, mean, std)\n\n[11/18/24 15:55:56] INFO     Loading data from params:normal_mean (MemoryDataset)...            data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_std (MemoryDataset)...             data_catalog.py:389\n\n\n\n\nresized_img.shape\n\n\n\n\n\ntorch.Size([3, 224, 224])\n\n\n\n\nresized_img \n\n\n\n\n\ntensor([[[1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9490, 0.9490],\n         [1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9490, 0.9490],\n         [1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9490, 0.9490],\n         ...,\n         [0.9373, 0.9373, 0.9373,  ..., 0.9020, 0.9020, 0.9020],\n         [0.9373, 0.9373, 0.9373,  ..., 0.9020, 0.9020, 0.9020],\n         [0.9373, 0.9373, 0.9373,  ..., 0.9020, 0.9020, 0.9020]],\n        [[0.6745, 0.6745, 0.6745,  ..., 0.6549, 0.6549, 0.6549],\n         [0.6745, 0.6745, 0.6745,  ..., 0.6549, 0.6549, 0.6549],\n         [0.6745, 0.6745, 0.6745,  ..., 0.6549, 0.6549, 0.6549],\n         ...,\n         [0.6353, 0.6353, 0.6353,  ..., 0.6157, 0.6157, 0.6157],\n         [0.6353, 0.6353, 0.6353,  ..., 0.6157, 0.6157, 0.6157],\n         [0.6353, 0.6353, 0.6353,  ..., 0.6157, 0.6157, 0.6157]],\n        [[0.7020, 0.7020, 0.7020,  ..., 0.6824, 0.6824, 0.6824],\n         [0.7020, 0.7020, 0.7020,  ..., 0.6824, 0.6824, 0.6824],\n         [0.7020, 0.7020, 0.7020,  ..., 0.6824, 0.6824, 0.6824],\n         ...,\n         [0.6039, 0.6039, 0.6039,  ..., 0.5922, 0.5922, 0.5922],\n         [0.6039, 0.6039, 0.6039,  ..., 0.5922, 0.5922, 0.5922],\n         [0.6039, 0.6039, 0.6039,  ..., 0.5922, 0.5922, 0.5922]]])\n\n\n\n\noutput\n\n\n\n\n\ntensor([[[2.2489, 2.2489, 2.2489,  ..., 2.0263, 2.0263, 2.0263],\n         [2.2489, 2.2489, 2.2489,  ..., 2.0263, 2.0263, 2.0263],\n         [2.2489, 2.2489, 2.2489,  ..., 2.0263, 2.0263, 2.0263],\n         ...,\n         [1.9749, 1.9749, 1.9749,  ..., 1.8208, 1.8208, 1.8208],\n         [1.9749, 1.9749, 1.9749,  ..., 1.8208, 1.8208, 1.8208],\n         [1.9749, 1.9749, 1.9749,  ..., 1.8208, 1.8208, 1.8208]],\n        [[0.9755, 0.9755, 0.9755,  ..., 0.8880, 0.8880, 0.8880],\n         [0.9755, 0.9755, 0.9755,  ..., 0.8880, 0.8880, 0.8880],\n         [0.9755, 0.9755, 0.9755,  ..., 0.8880, 0.8880, 0.8880],\n         ...,\n         [0.8004, 0.8004, 0.8004,  ..., 0.7129, 0.7129, 0.7129],\n         [0.8004, 0.8004, 0.8004,  ..., 0.7129, 0.7129, 0.7129],\n         [0.8004, 0.8004, 0.8004,  ..., 0.7129, 0.7129, 0.7129]],\n        [[1.3154, 1.3154, 1.3154,  ..., 1.2282, 1.2282, 1.2282],\n         [1.3154, 1.3154, 1.3154,  ..., 1.2282, 1.2282, 1.2282],\n         [1.3154, 1.3154, 1.3154,  ..., 1.2282, 1.2282, 1.2282],\n         ...,\n         [0.8797, 0.8797, 0.8797,  ..., 0.8274, 0.8274, 0.8274],\n         [0.8797, 0.8797, 0.8797,  ..., 0.8274, 0.8274, 0.8274],\n         [0.8797, 0.8797, 0.8797,  ..., 0.8274, 0.8274, 0.8274]]])\n\n\n\n\nimport mlflow\n\nmlflow.set_tracking_uri('http://localhost:5000')\nmodel = mlflow.pytorch.load_model(catalog.load(\"best_model_uri\"))\n\n[11/18/24 16:04:40] INFO     Loading data from best_model_uri (TextDataset)...                  data_catalog.py:389\n\n\n\n\n\n\n\nmodel(output.unsqueeze(0))\n\n\n\n\n\ntensor([[-6.9457, -7.5042, -5.5099, -6.1375, -3.4725, 10.2126, -4.6117]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\ndetector(output.unsqueeze(0).to(\"cuda\"))\n\n\n\n\n\ntensor([-2.9780e-05], device='cuda:0', grad_fn=&lt;MinBackward0&gt;)\n\n\n\n\noutput.device\n\n\n\n\n\ndevice(type='cpu')\n\n\n\n\nfrom torchvision.datasets import CIFAR10\n\n\ncifar10 = CIFAR10(root=\"~/.data\", download=True)\n\nFiles already downloaded and verified\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimg, label = cifar10[5126]\nplt.imshow(np.array(img))\n\n\n\n\n\n&lt;matplotlib.image.AxesImage object at 0x7f4d381fbd10&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndetector(normalize_image(resize_image(img, size), mean, std).unsqueeze(0).to(\"cuda\"))\n\n\n\n\n\ntensor([0.0001], device='cuda:0', grad_fn=&lt;MinBackward0&gt;)\n\n\n\n\n%reload_kedro\n\n[11/18/24 22:36:36] INFO     Resolved project path as: /home/saimadhavang/sem7/mlpe/onco-derm-ai.   __init__.py:178\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n                    INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/18/24 22:36:37] INFO     Kedro project onco-derm-ai                                             __init__.py:144\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:145\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:151\n\n\n\n\n%load_node ood_prepare_data\n\n                    WARNING  /home/saimadhavang/sem7/mlpe/onco-derm-ai/.od_env/lib/python3.12/site- warnings.py:110\n                             packages/kedro/ipython/__init__.py:319: UserWarning: This is an                       \n                             experimental feature, only Jupyter Notebook (&gt;7.0), Jupyter Lab,                      \n                             IPython, and VSCode Notebook are supported. If you encounter                          \n                             unexpected behaviour or would like to suggest feature enhancements,                   \n                             add it under this github issue                                                        \n                             https://github.com/kedro-org/kedro/issues/3580                                        \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    INFO     Loading node definition from                                           __init__.py:360\n                             /home/saimadhavang/sem7/mlpe/onco-derm-ai/src/onco_derm_ai/pipelines/o                \n                             od_detection/nodes.py                                                                 \n\n\n\n\n# Prepare necessary inputs for debugging\n# All debugging inputs must be defined in your project catalog\nout_ds = catalog.load(\"params:ood_detection_out_ds\")\nsize = catalog.load(\"params:img_size\")\nnormal_mean = catalog.load(\"params:normal_mean\")\nnormal_std = catalog.load(\"params:normal_std\")\n\nfrom typing import Tuple\nimport medmnist\nimport mlflow\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom medmnist import INFO\nfrom pytorch_ood.detector import RMD, MaxSoftmax, MultiMahalanobis\nfrom pytorch_ood.model import WideResNet\nfrom pytorch_ood.utils import OODMetrics, ToUnknown\nfrom torch.utils.data import ConcatDataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom tqdm import tqdm\n\ndef prepare_data(\n    out_ds: str,\n    size: Tuple[int, int],\n    normal_mean: Tuple[float, float, float],\n    normal_std: Tuple[float, float, float],\n) -&gt; Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    \"\"\"\n    Prepare the data for the OOD detection pipeline.\n\n    Args:\n        out_ds: The dataset to use as the OOD dataset.\n        size: The size to resize the images to.\n        normal_mean: The mean to normalize the images.\n        normal_std: The standard deviation to normalize the images.\n\n    Returns:\n        The in-distribution dataset and the out-of-distribution dataset.\n    \"\"\"\n    # transform = transforms.Compose(\n    #     [\n    #         transforms.Resize(size),\n    #         transforms.ToTensor(),\n    #         transforms.Normalize(normal_mean, normal_std),\n    #     ]\n    # )\n\n    transform = WideResNet.transform_for(\"cifar10-pt\")\n\n    info = INFO[\"dermamnist\"]\n    DataClass = getattr(medmnist, info[\"python_class\"])\n    dataset_in_train = DataClass(split=\"train\", download=True, transform=transform)\n    dataset_in_val = DataClass(split=\"val\", download=True, transform=transform)\n    dataset_in_test = DataClass(split=\"test\", download=True, transform=transform)\n    if out_ds == \"cifar10\":\n        dataset_out_test = CIFAR10(\n            root=\"~/.data\",\n            download=True,\n            transform=transform,\n            target_transform=ToUnknown(),\n        )\n    else:\n        raise ValueError(\"Invalid out_ds\")\n\n    in_dataset_test = ConcatDataset([dataset_in_val, dataset_in_test])\n\n    return dataset_in_train, in_dataset_test, dataset_out_test\n\n\nin_train, in__test, out_ds = prepare_data(out_ds, size, normal_mean, normal_std)\n\n[11/18/24 22:37:09] INFO     Loading data from params:ood_detection_out_ds (MemoryDataset)...   data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:img_size (MemoryDataset)...               data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_mean (MemoryDataset)...            data_catalog.py:389\n\n\n\n                    INFO     Loading data from params:normal_std (MemoryDataset)...             data_catalog.py:389\n\n\n\nUsing downloaded and verified file: /home/saimadhavang/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/saimadhavang/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/saimadhavang/.medmnist/dermamnist.npz\nFiles already downloaded and verified\n\n\n\nin_scores = []\nout_scores = []\n\nfrom torch.utils.data import DataLoader, ConcatDataset\n\nin_ds = ConcatDataset([in_train, in__test])\n\n\nin_loader = DataLoader(in_ds, batch_size=128, shuffle=True)\nout_loader = DataLoader(out_ds, batch_size=128, shuffle=True)\n\n\nfor batch in tqdm(in_loader):\n    in_scores.extend(detector(batch[0].to(\"cuda\")).detach().cpu().numpy())\n\n100%|██████████| 79/79 [00:05&lt;00:00, 13.85it/s]\n\n\n\nfor batch in tqdm(out_loader):\n    out_scores.extend(detector(batch[0].to(\"cuda\")).detach().cpu().numpy())\n\n100%|██████████| 391/391 [00:24&lt;00:00, 16.10it/s]\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.hist(in_scores, bins=500, alpha=0.5, label=\"In-distribution\")\nplt.hist(out_scores, bins=1000, alpha=0.5, label=\"Out-of-distribution\")\nplt.xlim(-0.1, 0.1)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nin_scores = np.array(in_scores)\nout_scores = np.array(out_scores)\n\n\nthreshold = 0.07\n(in_scores &gt; threshold).sum() / len(in_scores), (out_scores &gt; threshold).sum() / len(out_scores)\n\n\n\n\n\n(np.float64(0.01727408886669995), np.float64(0.98474))"
  },
  {
    "objectID": "reference/tensoring_resizing.html",
    "href": "reference/tensoring_resizing.html",
    "title": "tensoring_resizing",
    "section": "",
    "text": "tensoring_resizing\ntensoring_resizing(data)\nApplies a series of transformations to the ‘image’ column of a pandas DataFrame.\nThe transformations include converting images to PIL format, resizing them to 28x28 pixels, and converting them to tensors. The transformed images are then permuted and converted back to numpy arrays.\nArgs: data (pd.DataFrame): A pandas DataFrame containing an ‘image’ column with image data.\nReturns: pd.DataFrame: The input DataFrame with the ‘image’ column transformed."
  },
  {
    "objectID": "reference/log_model.html",
    "href": "reference/log_model.html",
    "title": "log_model",
    "section": "",
    "text": "log_model\nlog_model(model_name, model_state_dict, hyperparams, metrics, loss_plot)\nLogs the model, hyperparameters, and metrics to MLFlow.\nArgs: model_name (str): The name of the model. model_state_dict (dict): The state dictionary of the model. hyperparams (dict): The hyperparameters used during training. metrics (dict): The evaluation metrics of the model. loss_plot (plt.Figure): The plot of the training loss.\nReturns: str: The URI of the logged model."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the data_preprocessing pipeline\n\n\n\nnormalizing_images\nNormalizes the pixel values of images in the given DataFrame.\n\n\ntensoring_resizing\nApplies a series of transformations to the ‘image’ column of a pandas DataFrame.\n\n\n\n\n\n\nFunctions in the model_training pipeline\n\n\n\nDermaMNISTDataset\nA custom Dataset class for the DermaMNIST dataset.\n\n\npreprocess_data_input\nPreprocesses the input training data for the DermaMNIST dataset.\n\n\nmodel_select\nSelects and returns a pre-trained model based on the provided model name.\n\n\nmodel_finetune\nFinetunes a pre-trained model on the given training dataset.\n\n\nevaluate_model\nEvaluates a pre-trained model on the given test dataset.\n\n\nlog_model\nLogs the model, hyperparameters, and metrics to MLFlow."
  },
  {
    "objectID": "reference/index.html#data_preprocessing",
    "href": "reference/index.html#data_preprocessing",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the data_preprocessing pipeline\n\n\n\nnormalizing_images\nNormalizes the pixel values of images in the given DataFrame.\n\n\ntensoring_resizing\nApplies a series of transformations to the ‘image’ column of a pandas DataFrame."
  },
  {
    "objectID": "reference/index.html#model_training",
    "href": "reference/index.html#model_training",
    "title": "API Reference",
    "section": "",
    "text": "Functions in the model_training pipeline\n\n\n\nDermaMNISTDataset\nA custom Dataset class for the DermaMNIST dataset.\n\n\npreprocess_data_input\nPreprocesses the input training data for the DermaMNIST dataset.\n\n\nmodel_select\nSelects and returns a pre-trained model based on the provided model name.\n\n\nmodel_finetune\nFinetunes a pre-trained model on the given training dataset.\n\n\nevaluate_model\nEvaluates a pre-trained model on the given test dataset.\n\n\nlog_model\nLogs the model, hyperparameters, and metrics to MLFlow."
  },
  {
    "objectID": "reference/normalizing_images.html",
    "href": "reference/normalizing_images.html",
    "title": "normalizing_images",
    "section": "",
    "text": "normalizing_images\nnormalizing_images(data)\nNormalizes the pixel values of images in the given DataFrame.\nThis function takes a DataFrame containing image data and normalizes the pixel values by dividing each pixel value by 255.0. The normalized pixel values will be in the range [0, 1].\nArgs: data (pd.DataFrame): A DataFrame containing image data. The DataFrame must have a column named “image” where each entry is an image represented as a numerical array.\nReturns: pd.DataFrame: A DataFrame with the same structure as the input, but with normalized image pixel values."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "OncoDerm AI",
    "section": "",
    "text": "Project Proposal\n\nExecutive Summary\nWe propose developing a machine learning-based skin cancer screening system utilizing dermatoscopic images. The system will assist medical professionals in preliminary skin lesion assessment while incorporating robust MLOps practices and responsible AI principles.\n\n\nProblem Statement\nSkin cancer diagnosis requires expert dermatological knowledge and careful image analysis. While machine learning can assist in this process, deploying such systems in clinical settings requires careful consideration of reliability, explainability, and operational excellence.\n\n\nDataset\n\nSource: DermaMNIST (based on HAM10000)\nClasses: 7 distinct skin lesion categories:\n\nActinic keratoses and intraepithelial carcinoma (akiec)\nBasal cell carcinoma (bcc)\nBenign keratosis-like lesions (bkl)\nDermatofibroma (df)\nMelanoma (mel)\nMelanocytic nevi (nv)\nVascular lesions (vasc)\n\nCharacteristics:\n\n28x28 pixel dermatoscopic images\n10,015 training images\n1,268 validation images\n2,239 test images\n\nData Split: Predefined splits provided by MedMNIST\n\n\n\nTechnical Architecture\n\n1. Model Development\n\nBase Architecture:\n\nResNet-18 or MobileNetV2 (modified for 28x28 input)\nConsideration for lightweight models due to smaller input size\n\nTraining Pipeline:\n\nData augmentation (rotation, flipping, color jittering)\nTransfer learning with ImageNet weights\nFine-tuning strategies for small image sizes\nCross-validation for robust performance estimation\n\n\n\n\n2. MLOps Infrastructure\n\nData Pipeline:\n\nData ingestion and preprocessing\nData versioning\nData quality checks\n\nExperiment Tracking:\n\nMLflow for model versioning and tracking\nHyperparameter optimization\nModel performance visualization\n\nCI/CD Pipeline:\n\nAutomated testing (unit, integration, model performance)\nAutomated model deployment\nAutomated documentation generation\n\nMonitoring:\n\nModel performance metrics\nData drift detection\nAutomatic retraining triggers\n\n\n\n\n3. Production Features\n\nModel Robustness & Reliability\n\nExplainability:\nConfidence Calibration:\nAdversarial Robustness:\nOut-of-Distribution Detection:\n\n\n\nClinical Integration\n\nInteractive Dashboard:\n\nReal-time inference results\nConfidence scores and explanations\nImage preprocessing and quality checks\nResolution handling and upscaling options\n\nConformal Predictions:\n\nSet-valued predictions with guaranteed coverage\nCalibrated confidence scores\n\n\n\n\nData Privacy & Compliance\n\nRight to Erasure:\n\nAutomated removal pipeline\n\n\n\n\n\n\nEvaluation Metrics\n\nTechnical Metrics\n\nModel accuracy\nPrecision, recall, F1-score per class\nInference latency\nData drift metrics\n\n\n\nClinical Metrics\n\nFalse positive/negative rates\nCalibration error\nOOD detection accuracy\nExplanation quality (user feedback)\n\n\n\n\nChallenges & Risks\n\nTechnical Risks:\n\nLimited resolution impact on performance\nModel bias\nSystem scalability\nIntegration challenges\n\nClinical Risks:\n\nOver-reliance on system\nMisinterpretation of results\nEdge case handling\nResolution limitations affecting diagnosis\n\n\n\n\nMitigation Strategies\n\nClear Disclaimer: System is for screening assistance only\nResolution Warning: Clear indication of image resolution limitations\nComprehensive Documentation: Usage guidelines and limitations\nRegular Updates: Continuous model improvement\nUser Training: Proper system usage and interpretation\n\n\n\nFuture Enhancements\n\nVQA chatbot for assisting medical professionals\nExtension to other dermoscopic datasets"
  },
  {
    "objectID": "notebooks/DermaMnist.html",
    "href": "notebooks/DermaMnist.html",
    "title": "Importing data from medmnist",
    "section": "",
    "text": "To configure the data catalog, run the below cells. These cells will download the data from the medmnist dataset and save it in the data directory as a dataframe encapsuled in a pickle file.\n\n# !pip install medmnist\n\n\nimport numpy as np\nfrom medmnist import DermaMNIST\n\n\ndata_train = DermaMNIST(split=\"train\", download=True, root=\"../data/01_raw/\")\n\nUsing downloaded and verified file: ../data/01_raw/dermamnist.npz\n\n\n\nds = np.load(\"../data/01_raw/dermamnist.npz\")\n\n\ntrain_images = ds[\"train_images\"]\ntrain_labels = ds[\"train_labels\"]\nval_images = ds[\"val_images\"]\nval_labels = ds[\"val_labels\"]\ntest_images = ds[\"test_images\"]\ntest_labels = ds[\"test_labels\"]\n\n\n# def get_class_imbalance():\n#     global train_labels, val_labels, test_labels\n\n#     # Combine all labels to assess overall imbalance\n#     train_labels_list = train_labels.ravel().tolist()\n#     val_labels_list = val_labels.ravel().tolist()\n#     test_labels_list = test_labels.ravel().tolist()\n#     all_labels = np.concatenate([train_labels_list, val_labels_list, test_labels_list])\n\n#     # Function to calculate class distribution\n#     def calculate_class_distribution(labels):\n#         print(type(labels))\n#         class_counts = Counter(labels)  # Count occurrences of each class\n#         total_samples = len(labels)\n#         class_distribution = {\n#             cls: count / total_samples for cls, count in class_counts.items()\n#         }  # Normalize\n#         return class_counts, class_distribution\n\n#     # Function to calculate imbalance ratio\n#     def calculate_imbalance_ratio(class_counts):\n#         min_class_count = min(class_counts.values())\n#         max_class_count = max(class_counts.values())\n#         return max_class_count / min_class_count\n\n#     # Function to calculate entropy of class distribution\n#     def calculate_class_entropy(class_distribution):\n#         return -sum(p * np.log2(p) for p in class_distribution.values() if p &gt; 0)\n\n#     # Calculate metrics for train, val, and test sets\n#     train_counts, train_distribution = calculate_class_distribution(train_labels_list)\n#     val_counts, val_distribution = calculate_class_distribution(val_labels_list)\n#     test_counts, test_distribution = calculate_class_distribution(test_labels_list)\n#     overall_counts, overall_distribution = calculate_class_distribution(all_labels)\n\n#     # Imbalance ratios\n#     train_imbalance = calculate_imbalance_ratio(train_counts)\n#     val_imbalance = calculate_imbalance_ratio(val_counts)\n#     test_imbalance = calculate_imbalance_ratio(test_counts)\n\n#     # Class entropies\n#     train_entropy = calculate_class_entropy(train_distribution)\n#     val_entropy = calculate_class_entropy(val_distribution)\n#     test_entropy = calculate_class_entropy(test_distribution)\n\n#     # Display results\n#     print(\"Train Set:\")\n#     print(f\"Class Counts: {train_counts}\")\n#     print(f\"Imbalance Ratio: {train_imbalance:.2f}\")\n#     print(f\"Entropy: {train_entropy:.2f}\\n\")\n\n#     print(\"Validation Set:\")\n#     print(f\"Class Counts: {val_counts}\")\n#     print(f\"Imbalance Ratio: {val_imbalance:.2f}\")\n#     print(f\"Entropy: {val_entropy:.2f}\\n\")\n\n#     print(\"Test Set:\")\n#     print(f\"Class Counts: {test_counts}\")\n#     print(f\"Imbalance Ratio: {test_imbalance:.2f}\")\n#     print(f\"Entropy: {test_entropy:.2f}\\n\")\n\n#     # Plot class distribution\n#     def plot_class_distribution(class_counts, title):\n#         classes = list(class_counts.keys())\n#         counts = list(class_counts.values())\n#         plt.bar(classes, counts, color=\"skyblue\")\n#         plt.xlabel(\"Class\")\n#         plt.ylabel(\"Count\")\n#         plt.title(title)\n#         plt.show()\n\n#     plot_class_distribution(train_counts, \"Train Set Class Distribution\")\n#     plot_class_distribution(val_counts, \"Validation Set Class Distribution\")\n#     plot_class_distribution(test_counts, \"Test Set Class Distribution\")\n\n\n# import numpy as np\n# get_class_imbalance()\n\n\nfrom imblearn.over_sampling import SMOTE\n\n# print(train_images.shape)\nX = train_images.reshape(train_images.shape[0], -1)  # Flatten images\ny = train_labels\n\n(7007, 28, 28, 3)\n\n\n\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n\nX_resampled.shape\n\n\n\n\n\n(32851, 2352)\n\n\n\n\nX_resampled.reshape(-1, 28, 28, 3).shape\n\n\n\n\n\n(32851, 28, 28, 3)\n\n\n\n\ny_resampled.shape\n\n\n\n\n\n(32851,)\n\n\n\n\n# print(X_resampled[0])\n\n[158 111 117 ... 173 145 142]\n\n\n\ntrain_images = X_resampled.reshape(\n    -1, 28, 28, 3\n)  # Replace 28, 28 with actual dimensions\ntrain_labels = y_resampled\n# print(len(train_images))\n\n32851\n\n\n\n# get_class_imbalance()\n\n\ntrain_len = len(train_images)\nval_len = len(val_images)\ntest_len = len(test_images)\n# print(train_len)\n\n32851\n\n\n\n# generate ids array\ntrain_ids = [f\"train_{i}\" for i in range(train_len)]\nval_ids = [f\"val_{i}\" for i in range(val_len)]\ntest_ids = [f\"test_{i}\" for i in range(test_len)]\n\n\ntrain_images = list(train_images)\nval_images = list(val_images)\ntest_images = list(test_images)\n\n\ntrain_labels = list(train_labels)\nval_labels = list(val_labels)\ntest_labels = list(test_labels)\n\n\n# print(len(train_images))\n# print(len(train_labels))\n# print(len(train_ids))\n\n32851\n32851\n32851\n\n\n\n# construct a df for each of the splits\nimport pandas as pd\n\ntrain_df = pd.DataFrame(\n    {\n        \"id\": train_ids,\n        \"image\": train_images,\n        \"label\": train_labels,\n    }\n)\ntest_df = pd.DataFrame(\n    {\n        \"id\": test_ids,\n        \"image\": test_images,\n        \"label\": test_labels,\n    }\n)\nval_df = pd.DataFrame(\n    {\n        \"id\": val_ids,\n        \"image\": val_images,\n        \"label\": val_labels,\n    }\n)\n\n\n# train_df.to_pickle(\"../data/01_raw/train.pkl\")\n# test_df.to_pickle(\"../data/01_raw/test.pkl\")\n# val_df.to_pickle(\"../data/01_raw/val.pkl\")\n\n\ntrain_df_loaded = pd.read_pickle(\"../data/01_raw/train.pkl\")\n\n\ntrain_df_loaded[\"image\"].iloc[0].shape\n\n(28, 28, 3)\n\n\n\ndef normalizing_images(data: pd.DataFrame) -&gt; pd.DataFrame:\n    data[\"image\"] = data[\"image\"].apply(lambda x: x / 255.0)\n    return data\n\n\ntrain_df_loaded_new = normalizing_images(train_df_loaded)\ntrain_df_loaded_new\n\n\n\n\n\n\n\n\nid\nimage\nlabel\n\n\n\n\n0\ntrain_0\n[[[0.6196078431372549, 0.43529411764705883, 0....\n[0]\n\n\n1\ntrain_1\n[[[0.9019607843137255, 0.43529411764705883, 0....\n[5]\n\n\n2\ntrain_2\n[[[0.8980392156862745, 0.611764705882353, 0.67...\n[5]\n\n\n3\ntrain_3\n[[[0.8941176470588236, 0.4980392156862745, 0.4...\n[5]\n\n\n4\ntrain_4\n[[[0.8470588235294118, 0.7372549019607844, 0.7...\n[4]\n\n\n...\n...\n...\n...\n\n\n7002\ntrain_7002\n[[[0.788235294117647, 0.4980392156862745, 0.56...\n[5]\n\n\n7003\ntrain_7003\n[[[0.8941176470588236, 0.47843137254901963, 0....\n[5]\n\n\n7004\ntrain_7004\n[[[0.0784313725490196, 0.07058823529411765, 0....\n[2]\n\n\n7005\ntrain_7005\n[[[0.6705882352941176, 0.5529411764705883, 0.6...\n[5]\n\n\n7006\ntrain_7006\n[[[0.9372549019607843, 0.611764705882353, 0.69...\n[5]\n\n\n\n\n7007 rows × 3 columns\n\n\n\n\nfrom torchvision import transforms\n\n\ndef tensoring_resizing(data: pd.DataFrame) -&gt; pd.DataFrame:\n    transform = transforms.Compose(\n        [transforms.ToPILImage(), transforms.Resize((28, 28)), transforms.ToTensor()]\n    )\n\n    data[\"image\"] = data[\"image\"].apply(lambda x: transform(x).permute(1, 2, 0).numpy())\n    return data\n\n\ntrain_df_loaded_new = tensoring_resizing(train_df_loaded_new)\ntrain_df_loaded_new[\"image\"].iloc[0].shape\n\n(28, 28, 3)\n\n\n\n%load_ext kedro.ipython\n\n[11/18/24 23:20:28] INFO     Using                                                                  __init__.py:249\n                             'c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\framewor                \n                             k\\project\\rich_logging.yml' as logging configuration.                                 \n\n\n\n                    INFO     Registered line magic '%reload_kedro'                                   __init__.py:58\n\n\n\n                    INFO     Registered line magic '%load_node'                                      __init__.py:60\n\n\n\n                    INFO     Resolved project path as: c:\\Users\\Admin\\Desktop\\onco-derm-ai.         __init__.py:175\n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[11/18/24 23:20:31] INFO     Registering new custom resolver: 'km.random_name'                    mlflow_hook.py:62\n\n\n\n                    WARNING  No 'mlflow.yml' config file found in environment. Default            mlflow_hook.py:75\n                             configuration will be used. Use ``kedro mlflow init`` command in CLI                  \n                             to customize the configuration.                                                       \n\n\n\n                    INFO     The 'tracking_uri' key in mlflow.yml is relative            kedro_mlflow_config.py:260\n                             ('server.mlflow_(tracking|registry)_uri = mlruns'). It is                             \n                             converted to a valid uri:                                                             \n                             'file:///C:/Users/Admin/Desktop/onco-derm-ai/mlruns'                                  \n\n\n\n                    WARNING  Malformed experiment '312469223918544298'. Detailed error Yaml file  file_store.py:331\n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n                             ╭─────────────── Traceback (most recent call last) ────────────────╮                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:327 in search_experiments             │                  \n                             │                                                                  │                  \n                             │    324 │   │   for exp_id in experiment_ids:                     │                  \n                             │    325 │   │   │   try:                                          │                  \n                             │    326 │   │   │   │   # trap and warn known issues, will raise  │                  \n                             │ ❱  327 │   │   │   │   exp = self._get_experiment(exp_id, view_t │                  \n                             │    328 │   │   │   │   if exp is not None:                       │                  \n                             │    329 │   │   │   │   │   experiments.append(exp)               │                  \n                             │    330 │   │   │   except MissingConfigException as e:           │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:421 in _get_experiment                │                  \n                             │                                                                  │                  \n                             │    418 │   │   │   │   f\"Could not find experiment with ID {expe │                  \n                             │    419 │   │   │   │   databricks_pb2.RESOURCE_DOES_NOT_EXIST,   │                  \n                             │    420 │   │   │   )                                             │                  \n                             │ ❱  421 │   │   meta = FileStore._read_yaml(experiment_dir, FileS │                  \n                             │    422 │   │   meta[\"tags\"] = self.get_all_experiment_tags(exper │                  \n                             │    423 │   │   experiment = _read_persisted_experiment_dict(meta │                  \n                             │    424 │   │   if experiment_id != experiment.experiment_id:     │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1367 in _read_yaml                    │                  \n                             │                                                                  │                  \n                             │   1364 │   │   │   │   time.sleep(0.1 * (3 - attempts_remaining) │                  \n                             │   1365 │   │   │   │   return _read_helper(root, file_name, atte │                  \n                             │   1366 │   │                                                     │                  \n                             │ ❱ 1367 │   │   return _read_helper(root, file_name, attempts_rem │                  \n                             │   1368 │                                                         │                  \n                             │   1369 │   def _get_traces_artifact_dir(self, experiment_id, req │                  \n                             │   1370 │   │   return append_to_uri_path(                        │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1360 in _read_helper                  │                  \n                             │                                                                  │                  \n                             │   1357 │   │   \"\"\"                                               │                  \n                             │   1358 │   │                                                     │                  \n                             │   1359 │   │   def _read_helper(root, file_name, attempts_remain │                  \n                             │ ❱ 1360 │   │   │   result = read_yaml(root, file_name)           │                  \n                             │   1361 │   │   │   if result is not None or attempts_remaining = │                  \n                             │   1362 │   │   │   │   return result                             │                  \n                             │   1363 │   │   │   else:                                         │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\ut │                  \n                             │ ils\\file_utils.py:309 in read_yaml                               │                  \n                             │                                                                  │                  \n                             │    306 │                                                         │                  \n                             │    307 │   file_path = os.path.join(root, file_name)             │                  \n                             │    308 │   if not exists(file_path):                             │                  \n                             │ ❱  309 │   │   raise MissingConfigException(f\"Yaml file '{file_p │                  \n                             │    310 │   with codecs.open(file_path, mode=\"r\", encoding=ENCODI │                  \n                             │    311 │   │   return yaml.load(yaml_file, Loader=YamlSafeLoader │                  \n                             │    312                                                           │                  \n                             ╰──────────────────────────────────────────────────────────────────╯                  \n                             MissingConfigException: Yaml file                                                     \n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n\n\n\n[11/18/24 23:20:32] WARNING  Malformed experiment '312469223918544298'. Detailed error Yaml file  file_store.py:331\n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n                             ╭─────────────── Traceback (most recent call last) ────────────────╮                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:327 in search_experiments             │                  \n                             │                                                                  │                  \n                             │    324 │   │   for exp_id in experiment_ids:                     │                  \n                             │    325 │   │   │   try:                                          │                  \n                             │    326 │   │   │   │   # trap and warn known issues, will raise  │                  \n                             │ ❱  327 │   │   │   │   exp = self._get_experiment(exp_id, view_t │                  \n                             │    328 │   │   │   │   if exp is not None:                       │                  \n                             │    329 │   │   │   │   │   experiments.append(exp)               │                  \n                             │    330 │   │   │   except MissingConfigException as e:           │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:421 in _get_experiment                │                  \n                             │                                                                  │                  \n                             │    418 │   │   │   │   f\"Could not find experiment with ID {expe │                  \n                             │    419 │   │   │   │   databricks_pb2.RESOURCE_DOES_NOT_EXIST,   │                  \n                             │    420 │   │   │   )                                             │                  \n                             │ ❱  421 │   │   meta = FileStore._read_yaml(experiment_dir, FileS │                  \n                             │    422 │   │   meta[\"tags\"] = self.get_all_experiment_tags(exper │                  \n                             │    423 │   │   experiment = _read_persisted_experiment_dict(meta │                  \n                             │    424 │   │   if experiment_id != experiment.experiment_id:     │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1367 in _read_yaml                    │                  \n                             │                                                                  │                  \n                             │   1364 │   │   │   │   time.sleep(0.1 * (3 - attempts_remaining) │                  \n                             │   1365 │   │   │   │   return _read_helper(root, file_name, atte │                  \n                             │   1366 │   │                                                     │                  \n                             │ ❱ 1367 │   │   return _read_helper(root, file_name, attempts_rem │                  \n                             │   1368 │                                                         │                  \n                             │   1369 │   def _get_traces_artifact_dir(self, experiment_id, req │                  \n                             │   1370 │   │   return append_to_uri_path(                        │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1360 in _read_helper                  │                  \n                             │                                                                  │                  \n                             │   1357 │   │   \"\"\"                                               │                  \n                             │   1358 │   │                                                     │                  \n                             │   1359 │   │   def _read_helper(root, file_name, attempts_remain │                  \n                             │ ❱ 1360 │   │   │   result = read_yaml(root, file_name)           │                  \n                             │   1361 │   │   │   if result is not None or attempts_remaining = │                  \n                             │   1362 │   │   │   │   return result                             │                  \n                             │   1363 │   │   │   else:                                         │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\ut │                  \n                             │ ils\\file_utils.py:309 in read_yaml                               │                  \n                             │                                                                  │                  \n                             │    306 │                                                         │                  \n                             │    307 │   file_path = os.path.join(root, file_name)             │                  \n                             │    308 │   if not exists(file_path):                             │                  \n                             │ ❱  309 │   │   raise MissingConfigException(f\"Yaml file '{file_p │                  \n                             │    310 │   with codecs.open(file_path, mode=\"r\", encoding=ENCODI │                  \n                             │    311 │   │   return yaml.load(yaml_file, Loader=YamlSafeLoader │                  \n                             │    312                                                           │                  \n                             ╰──────────────────────────────────────────────────────────────────╯                  \n                             MissingConfigException: Yaml file                                                     \n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n\n\n\n2024/11/18 23:20:32 INFO mlflow.tracking.fluent: Experiment with name 'onco_derm_ai' does not exist. Creating a new experiment.\n\n\n                    WARNING  Malformed experiment '312469223918544298'. Detailed error Yaml file  file_store.py:331\n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n                             ╭─────────────── Traceback (most recent call last) ────────────────╮                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:327 in search_experiments             │                  \n                             │                                                                  │                  \n                             │    324 │   │   for exp_id in experiment_ids:                     │                  \n                             │    325 │   │   │   try:                                          │                  \n                             │    326 │   │   │   │   # trap and warn known issues, will raise  │                  \n                             │ ❱  327 │   │   │   │   exp = self._get_experiment(exp_id, view_t │                  \n                             │    328 │   │   │   │   if exp is not None:                       │                  \n                             │    329 │   │   │   │   │   experiments.append(exp)               │                  \n                             │    330 │   │   │   except MissingConfigException as e:           │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:421 in _get_experiment                │                  \n                             │                                                                  │                  \n                             │    418 │   │   │   │   f\"Could not find experiment with ID {expe │                  \n                             │    419 │   │   │   │   databricks_pb2.RESOURCE_DOES_NOT_EXIST,   │                  \n                             │    420 │   │   │   )                                             │                  \n                             │ ❱  421 │   │   meta = FileStore._read_yaml(experiment_dir, FileS │                  \n                             │    422 │   │   meta[\"tags\"] = self.get_all_experiment_tags(exper │                  \n                             │    423 │   │   experiment = _read_persisted_experiment_dict(meta │                  \n                             │    424 │   │   if experiment_id != experiment.experiment_id:     │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1367 in _read_yaml                    │                  \n                             │                                                                  │                  \n                             │   1364 │   │   │   │   time.sleep(0.1 * (3 - attempts_remaining) │                  \n                             │   1365 │   │   │   │   return _read_helper(root, file_name, atte │                  \n                             │   1366 │   │                                                     │                  \n                             │ ❱ 1367 │   │   return _read_helper(root, file_name, attempts_rem │                  \n                             │   1368 │                                                         │                  \n                             │   1369 │   def _get_traces_artifact_dir(self, experiment_id, req │                  \n                             │   1370 │   │   return append_to_uri_path(                        │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1360 in _read_helper                  │                  \n                             │                                                                  │                  \n                             │   1357 │   │   \"\"\"                                               │                  \n                             │   1358 │   │                                                     │                  \n                             │   1359 │   │   def _read_helper(root, file_name, attempts_remain │                  \n                             │ ❱ 1360 │   │   │   result = read_yaml(root, file_name)           │                  \n                             │   1361 │   │   │   if result is not None or attempts_remaining = │                  \n                             │   1362 │   │   │   │   return result                             │                  \n                             │   1363 │   │   │   else:                                         │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\ut │                  \n                             │ ils\\file_utils.py:309 in read_yaml                               │                  \n                             │                                                                  │                  \n                             │    306 │                                                         │                  \n                             │    307 │   file_path = os.path.join(root, file_name)             │                  \n                             │    308 │   if not exists(file_path):                             │                  \n                             │ ❱  309 │   │   raise MissingConfigException(f\"Yaml file '{file_p │                  \n                             │    310 │   with codecs.open(file_path, mode=\"r\", encoding=ENCODI │                  \n                             │    311 │   │   return yaml.load(yaml_file, Loader=YamlSafeLoader │                  \n                             │    312                                                           │                  \n                             ╰──────────────────────────────────────────────────────────────────╯                  \n                             MissingConfigException: Yaml file                                                     \n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n\n\n\n[11/18/24 23:20:33] WARNING  Malformed experiment '312469223918544298'. Detailed error Yaml file  file_store.py:331\n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n                             ╭─────────────── Traceback (most recent call last) ────────────────╮                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:327 in search_experiments             │                  \n                             │                                                                  │                  \n                             │    324 │   │   for exp_id in experiment_ids:                     │                  \n                             │    325 │   │   │   try:                                          │                  \n                             │    326 │   │   │   │   # trap and warn known issues, will raise  │                  \n                             │ ❱  327 │   │   │   │   exp = self._get_experiment(exp_id, view_t │                  \n                             │    328 │   │   │   │   if exp is not None:                       │                  \n                             │    329 │   │   │   │   │   experiments.append(exp)               │                  \n                             │    330 │   │   │   except MissingConfigException as e:           │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:421 in _get_experiment                │                  \n                             │                                                                  │                  \n                             │    418 │   │   │   │   f\"Could not find experiment with ID {expe │                  \n                             │    419 │   │   │   │   databricks_pb2.RESOURCE_DOES_NOT_EXIST,   │                  \n                             │    420 │   │   │   )                                             │                  \n                             │ ❱  421 │   │   meta = FileStore._read_yaml(experiment_dir, FileS │                  \n                             │    422 │   │   meta[\"tags\"] = self.get_all_experiment_tags(exper │                  \n                             │    423 │   │   experiment = _read_persisted_experiment_dict(meta │                  \n                             │    424 │   │   if experiment_id != experiment.experiment_id:     │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1367 in _read_yaml                    │                  \n                             │                                                                  │                  \n                             │   1364 │   │   │   │   time.sleep(0.1 * (3 - attempts_remaining) │                  \n                             │   1365 │   │   │   │   return _read_helper(root, file_name, atte │                  \n                             │   1366 │   │                                                     │                  \n                             │ ❱ 1367 │   │   return _read_helper(root, file_name, attempts_rem │                  \n                             │   1368 │                                                         │                  \n                             │   1369 │   def _get_traces_artifact_dir(self, experiment_id, req │                  \n                             │   1370 │   │   return append_to_uri_path(                        │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\st │                  \n                             │ ore\\tracking\\file_store.py:1360 in _read_helper                  │                  \n                             │                                                                  │                  \n                             │   1357 │   │   \"\"\"                                               │                  \n                             │   1358 │   │                                                     │                  \n                             │   1359 │   │   def _read_helper(root, file_name, attempts_remain │                  \n                             │ ❱ 1360 │   │   │   result = read_yaml(root, file_name)           │                  \n                             │   1361 │   │   │   if result is not None or attempts_remaining = │                  \n                             │   1362 │   │   │   │   return result                             │                  \n                             │   1363 │   │   │   else:                                         │                  \n                             │                                                                  │                  \n                             │ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\mlflow\\ut │                  \n                             │ ils\\file_utils.py:309 in read_yaml                               │                  \n                             │                                                                  │                  \n                             │    306 │                                                         │                  \n                             │    307 │   file_path = os.path.join(root, file_name)             │                  \n                             │    308 │   if not exists(file_path):                             │                  \n                             │ ❱  309 │   │   raise MissingConfigException(f\"Yaml file '{file_p │                  \n                             │    310 │   with codecs.open(file_path, mode=\"r\", encoding=ENCODI │                  \n                             │    311 │   │   return yaml.load(yaml_file, Loader=YamlSafeLoader │                  \n                             │    312                                                           │                  \n                             ╰──────────────────────────────────────────────────────────────────╯                  \n                             MissingConfigException: Yaml file                                                     \n                             'C:\\Users\\Admin\\Desktop\\onco-derm-ai\\mlruns\\312469223918544298\\meta.                  \n                             yaml' does not exist.                                                                 \n\n\n\n                    INFO     Kedro is sending anonymous usage data with the sole purpose of improving plugin.py:233\n                             the product. No personal data or IP addresses are stored on our side. If              \n                             you want to opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK`              \n                             environment variables, or create a `.telemetry` file in the current                   \n                             working directory with the contents `consent: false`. Read more at                    \n                             https://docs.kedro.org/en/stable/configuration/telemetry.html                         \n\n\n\n[11/18/24 23:20:34] INFO     Kedro project onco-derm-ai                                             __init__.py:141\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:142\n                             'pipelines'                                                                           \n\n\n\n[11/18/24 23:20:35] INFO     Registered line magic 'run_viz'                                        __init__.py:148\n\n\n\n\nfrom kedro.io.data_catalog import DataCatalog\n\ncatalog = DataCatalog.from_config(\"../conf/base/catalog.yml\")\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[2], line 3\n      1 from kedro.io.data_catalog import DataCatalog\n----&gt; 3 catalog = DataCatalog.from_config(\"../conf/base/catalog.yml\")\n\nFile c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\data_catalog.py:314, in DataCatalog.from_config(cls, catalog, credentials, load_versions, save_version)\n    311 load_versions = copy.deepcopy(load_versions) or {}\n    312 user_default = {}\n--&gt; 314 for ds_name, ds_config in catalog.items():\n    315     if not isinstance(ds_config, dict):\n    316         raise DatasetError(\n    317             f\"Catalog entry '{ds_name}' is not a valid dataset configuration. \"\n    318             \"\\nHint: If this catalog entry is intended for variable interpolation, \"\n    319             \"make sure that the key is preceded by an underscore.\"\n    320         )\n\nAttributeError: 'str' object has no attribute 'items'\n\n\n\n\ncatalog.save(\"train_raw\", train_df)\n# catalog.save(\"test_raw\", test_df)\n# catalog.save(\"val_raw\", val_df)\n\n[11/18/24 23:22:45] INFO     Saving data to train_raw (PickleDataset)...                        data_catalog.py:581\n\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 catalog.save(\"train_raw\", train_df)                                                          │\n│   2 # catalog.save(\"test_raw\", test_df)                                                          │\n│   3 # catalog.save(\"val_raw\", val_df)                                                            │\n│   4                                                                                              │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\data_catalog.py:588 in save      │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\core.py:727 in save              │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\core.py:249 in save              │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro_datasets\\pickle\\pickle_dataset.py:2 │\n│ 25 in _save                                                                                      │\n│                                                                                                  │\n│ c:\\Users\\Admin\\miniconda3\\envs\\MlOps\\lib\\site-packages\\kedro\\io\\core.py:704 in _get_save_path    │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nDatasetError: Save path \n'C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/train.pkl/2024-11-18T17.50.33.688Z/train.pkl' for \nPickleDataset(backend=pickle, filepath=C:/Users/Admin/Desktop/onco-derm-ai/data/01_raw/train.pkl, load_args={}, \nprotocol=file, save_args={}, version=Version(load=None, save='2024-11-18T17.50.33.688Z')) must not exist if \nversioning is enabled.\n\n\n\n\ndf = catalog.load(\"train_raw\")\n\n[11/18/24 23:30:16] INFO     Loading data from train_raw (PickleDataset)...                     data_catalog.py:539\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n\n\n\nid\nimage\nlabel\n\n\n\n\n0\ntrain_0\n[[[158, 111, 117, 161, 116, 121, 164, 121, 130...\n0\n\n\n1\ntrain_1\n[[[230, 111, 105, 226, 107, 99, 225, 106, 98, ...\n5\n\n\n2\ntrain_2\n[[[229, 156, 173, 229, 156, 173, 227, 156, 174...\n5\n\n\n3\ntrain_3\n[[[228, 127, 115, 224, 126, 113, 223, 125, 112...\n5\n\n\n4\ntrain_4\n[[[216, 188, 187, 217, 189, 188, 220, 192, 191...\n4\n\n\n...\n...\n...\n...\n\n\n32846\ntrain_32846\n[[[220, 155, 157, 220, 154, 156, 222, 248, 155...\n6\n\n\n32847\ntrain_32847\n[[[37, 228, 147, 39, 230, 148, 39, 230, 238, 3...\n6\n\n\n32848\ntrain_32848\n[[[189, 161, 208, 189, 161, 208, 191, 160, 175...\n6\n\n\n32849\ntrain_32849\n[[[159, 107, 128, 246, 106, 128, 248, 103, 129...\n6\n\n\n32850\ntrain_32850\n[[[70, 39, 67, 69, 38, 66, 70, 38, 66, 71, 39,...\n6\n\n\n\n\n32851 rows × 3 columns\n\n\n\n\ndf[\"image\"][0].shape\n\n\n\n\n\n(3, 28, 28)\n\n\n\n\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndevice\n\ndevice(type='cuda')"
  }
]